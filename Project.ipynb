{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import numpy as np\n",
    "import re\n",
    "import os.path\n",
    "import glob\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "Overview: For Part 1, we downloaded the Parquet files, cleaned and filtered for the relevant data, filling in missing data, and generating samples of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones\n",
    "\n",
    "Downloading\n",
    "programmatically download the Yellow Taxi trip data. \n",
    "Using the re module, write a regular expression to help pull out the desired links for Yellow Taxi Parquet files. You are required to use the 3rd-party packages requests and  BeautifulSoup to programmatically download the Yellow Taxi Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "\n",
    "#Load taxi zones from a shapefile.\n",
    "    taxi_zones = gpd.read_file(shapefile)\n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "    if len(zone) == 0:\n",
    "        raise ValueError(f\"Taxi zone with LocationID {zone_loc_id} not found.\")\n",
    "    elif len(zone) > 1:\n",
    "        raise ValueError(f\"Multiple taxi zones found with LocationID {zone_loc_id}.\")\n",
    "    lat = zone.geometry.centroid.y.values[0]\n",
    "    lon = zone.geometry.centroid.x.values[0]\n",
    "    return lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance\n",
    "\n",
    "1.the rad(d) function converts numeric degrees to radians.\n",
    "2.the calculate_distance_with_coords(from_coord, to_coord) function calculate the distance btween from_coord and to_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a34ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts numeric degrees to radians\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    rad_Lat1 = rad(from_coord['pickup_latitude'])\n",
    "    rad_Lat2 = rad(to_coord['dropoff_latitude'])\n",
    "    a = rad_Lat1 - rad_Lat2\n",
    "    b = rad(from_coord['pickup_longitude']) - rad(to_coord['dropoff_longitude'])\n",
    "    distance = 2 * math.asin(math.sqrt(math.pow(math.sin(a.iloc[0] / 2), 2) \\\n",
    "                                       + math.cos(radLat1.iloc[0]) * math.cos(radLat2.iloc[0]) * math.pow(math.sin(b.iloc[0] / 2), 2)))\n",
    "    new_distance = distance * EARTH_REDIUS\n",
    "    return new_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7480c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_zones(from_zone, to_zone):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    from_coord = dataframe[['pickup_latitude', 'pickup_longitude']]\n",
    "    to_coord = dataframe[['dropoff_latitude', 'dropoff_longitude']]\n",
    "    dataframe['distance'] = calculate_distance(from_coord, to_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page():\n",
    "    url_list=[]\n",
    "    try:\n",
    "        response=requests.get(TAXI_URL)\n",
    "        if response.status_code==200:\n",
    "            soup=BeautifulSoup(response.content,'lxml')\n",
    "            hrefs=soup.find_all('a',href=re.compile(\"yellow_tripdata\"))\n",
    "            for href in hrefs:\n",
    "                url=href.get('href')\n",
    "                date = url.split('/')[-1].split(\"_\")[-1]\n",
    "                year = int(date[:4])\n",
    "                if year >= 2009 and year < 2015:\n",
    "                    url_list.append(url)\n",
    "                if year == 2015:\n",
    "                    month = int(date[5:7])\n",
    "                    if month <= 6:\n",
    "                        url_list.append(url) \n",
    "        return url_list\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294276b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"tpep_pickup_datetime\", \n",
    "           \"pickup_longitude\", \n",
    "           \"pickup_latitude\", \n",
    "           \"dropoff_longitude\", \n",
    "           \"dropoff_latitude\",\n",
    "           \"tip_amount\",\n",
    "           \"total_amount\"]\n",
    "columns2 = [\"pickup_datetime\", \n",
    "           \"pickup_longitude\", \n",
    "           \"pickup_latitude\", \n",
    "           \"dropoff_longitude\", \n",
    "           \"dropoff_latitude\",\n",
    "           \"tip_amount\",\n",
    "           \"total_amount\"]\n",
    "columns3 = [\"Trip_Pickup_DateTime\", \n",
    "           \"Start_Lon\",\n",
    "           \"Start_Lat\",\n",
    "           \"End_Lon\", \n",
    "           \"End_Lat\",\n",
    "           \"Tip_Amt\",\n",
    "           \"Total_Amt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_latlog_column(dataframe):\n",
    "    \"\"\" \n",
    "    Here, we added latitude and logitude from taxi_zones.\n",
    "    We also added pickup_latitude, dropoff_latitude, dropoff_latitude and dropoff_longitude as columns to the dataframe for convenient calculation\n",
    "    \"\"\"\n",
    "    print('add_latlog_column')\n",
    "    dftaxi = gpd.read_file('taxi_zones.shp')\n",
    "    dftaxi = dftaxi.to_crs(epsg=4326)\n",
    "    \n",
    "    lat1=[]\n",
    "    log1=[]\n",
    "    lat2=[]\n",
    "    log2=[]\n",
    "    for LocationID in dataframe[\"PULocationID\"]:\n",
    "        lat=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.x\n",
    "        log=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.y\n",
    "        if lat.empty:\n",
    "            lat1.append(0)\n",
    "        else:\n",
    "            lat1.append(lat.iloc[0])\n",
    "        if log.empty:\n",
    "            log1.append(0)\n",
    "        else:\n",
    "            log1.append(log.iloc[0])\n",
    "    for LocationID in dataframe[\"DOLocationID\"]:\n",
    "        lat=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.x\n",
    "        log=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.y\n",
    "        if lat.empty:\n",
    "            lat2.append(0)\n",
    "        else:\n",
    "            lat2.append(lat.iloc[0])\n",
    "        if log.empty:\n",
    "            log2.append(0)\n",
    "        else:\n",
    "            log2.append(log.iloc[0])\n",
    "    dataframe['pickup_latitude']=lat1\n",
    "    dataframe['pickup_longitude']=log1\n",
    "    dataframe['dropoff_latitude']=lat2\n",
    "    dataframe['dropoff_longitude']=log2\n",
    "    dataframe.to_csv(\"2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url):\n",
    "    \"\"\" \n",
    "    Download Parquet files, get some sample from these files\n",
    "    Clean the dataframe according to existing location IDs\n",
    "    Write data into .csv and return dataframe\n",
    "    \"\"\"\n",
    "    reponse = requests.get(url)\n",
    "\n",
    "    filename=url.split('/')[-1]\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(reponse.content)\n",
    "    \n",
    "    df = pd.read_parquet(filename)\n",
    "    print(filename)\n",
    "    print(df.columns)\n",
    "    df = df.sample(n=sample_size,ignore_index=True)\n",
    "    '''\n",
    "    df.columns = df.columns.str.strip()\n",
    "    print(df.columns)\n",
    "   \n",
    "    df=df.sample(n=sample_size,ignore_index=True)\n",
    "    add_latlog_column(df)\n",
    "    df = df[columns]\n",
    "    df.columns = columns2\n",
    "    df = df[columns2]\n",
    "    '''\n",
    "    try:\n",
    "        if \"PULocationID\" in df.columns:\n",
    "            add_latlog_column(df)\n",
    "        df = df[columns]\n",
    "    except:\n",
    "        try:\n",
    "            df = df[columns2]\n",
    "        except:\n",
    "            try:\n",
    "                df = df[columns3]\n",
    "            except:\n",
    "                add_latlog_column(df)\n",
    "                df = df[columns2]\n",
    "    df.columns = columns2\n",
    "    # df.to_csv(\"3.csv\")\n",
    "    #print(df[\"pickup_longitude\"])\n",
    "    '''\n",
    "    df=df[df[\"pickup_longitude\"] > -74.242330]\n",
    "    df = df[(df[\"pickup_longitude\"] > -74.242330) \n",
    "           &(df[\"pickup_longitude\"] < -73.717047)\n",
    "           & (df[\"pickup_latitude\"] > 40.560445) \n",
    "           & (df[\"pickup_latitude\"] < 40.908524)\n",
    "    '''\n",
    "    df = df.sample(n=sample_size,random_state = 1,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        add_distance_column(dataframe)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    return taxi_data\n",
    "\n",
    "    \"\"\" \n",
    "    Get taxi data. If taxi.csv exists, read-only. Otherwise, download data and generate taxi.csv file\n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    if os.path.exists(TAXI_CSV):\n",
    "        taxi_data=pd.read_csv(TAXI_CSV)\n",
    "    else:\n",
    "        all_csv_urls = find_taxi_csv_urls()\n",
    "        for csv_url in all_csv_urls:\n",
    "            dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "            add_distance_column(dataframe)\n",
    "            all_taxi_dataframes.append(dataframe)\n",
    "        print(all_taxi_dataframes)\n",
    "        taxi_data = pd.concat(all_taxi_dataframes)\n",
    "        taxi_data.to_csv(TAXI_CSV)\n",
    "    taxi_data[\"pickup_datetime\"] = pd.to_datetime(taxi_data['pickup_datetime'])\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = find_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_DATA)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
