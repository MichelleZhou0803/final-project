{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a606159",
   "metadata": {},
   "source": [
    "## Group 10 \n",
    "### Yixuan (Sharon) Qian - yq2348\n",
    "### Michelle Jingyi Zhou - jz3508"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import numpy as np\n",
    "import re\n",
    "import os.path\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants we might need\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV_FILES = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\",\n",
    "                    \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "\n",
    "EARTH_RADIUS = 6378.137\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "Overview: For Part 1, we downloaded the Parquet files, cleaned and filtered for the relevant data, filling in missing data, and generating samples of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "## I changed this whole part to Mardown for now, we might change the order and load the taxi zone later - Michelle\n",
    "\n",
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2178b34",
   "metadata": {},
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "\n",
    "#Load taxi zones from a shapefile.\n",
    "    taxi_zones = gpd.read_file(shapefile)\n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c907f",
   "metadata": {},
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "    if len(zone) == 0:\n",
    "        raise ValueError(f\"Taxi zone with LocationID {zone_loc_id} not found.\")\n",
    "    elif len(zone) > 1:\n",
    "        raise ValueError(f\"Multiple taxi zones found with LocationID {zone_loc_id}.\")\n",
    "    lat = zone.geometry.centroid.y.values[0]\n",
    "    lon = zone.geometry.centroid.x.values[0]\n",
    "    return lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance\n",
    "\n",
    "1.rad(d) function converts numeric degrees to radians\n",
    "\n",
    "2.distance calculation function\n",
    "calculate_distance_with_coords(from_coord, to_coord) calculates the distance btween coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a34ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts numeric degrees to radians\n",
    "# d is diameter\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    rad_lat1 = rad(from_coord['pickup_latitude'])\n",
    "    rad_lon1 = rad(from_coord['pickup_longitude'])\n",
    "    rad_lat2 = rad(to_coord['dropoff_longitude'])\n",
    "    rad_lon2 = rad(to_coord['dropoff_latitude'])\n",
    "    a = rad_lat1 - rad_lat2\n",
    "    b = rad_lon1 - rad_lon2\n",
    "    distance = 2 * math.asin(\n",
    "        math.sqrt(math.pow(math.sin(a / 2), 2)+ math.cos(rad_lat1) * math.cos(rad_lat2) * math.pow(math.sin(b / 2), 2)))\n",
    "    distance = distance * EARTH_RADIUS\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00abd1b",
   "metadata": {},
   "source": [
    "#### I commented out this cell - Michelle\n",
    "def calculate_distance_with_zones(from_zone, to_zone):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The calculation function is used to add columns to the relevant pandas dataframes (taxi data, uber data). \n",
    "\n",
    "def add_distance_column(dataframe):\n",
    "    from_coord = dataframe[['pickup_latitude', 'pickup_longitude']]\n",
    "    to_coord = dataframe[['dropoff_latitude', 'dropoff_longitude']]\n",
    "    dataframe['cal_distance'] = calculate_distance(from_coord, to_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data\n",
    "\n",
    "1. this function programmatically downloads the Yellow Taxi Parquet files for a specific date range 2009-01 and 2015-06 from the website. it returns a list that contains all taxi data url in TAXI_URL \"\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "3. we added latitude and logitude from taxi_zones. We also added pickup_latitude, dropoff_latitude, dropoff_latitude and dropoff_longitude as columns to the dataframe for convenient calculation\n",
    "\n",
    "4. Download Parquet files, get some sample from these files, Clean the dataframe according to existing location IDs, Write data into .csv and return dataframe\n",
    "\n",
    "5. get_and_clean_taxi_data\n",
    "Get taxi data. If taxi.csv exists, read-only. Otherwise, download data and generate taxi.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page():\n",
    "    parquet_url_list=[]\n",
    "    response=requests.get(url = TAXI_URL)\n",
    "    \n",
    "    if response.status_code==200:\n",
    "        soup=BeautifulSoup(response.content,'lxml')\n",
    "        hrefs=soup.find_all('a',href=re.compile(\"yellow_tripdata\"))\n",
    "        for href in hrefs:\n",
    "            url=href.get('href')\n",
    "            date = url.split('/')[-1].split(\"_\")[-1]\n",
    "            year = int(date[:4])\n",
    "            if year >= 2009 and year < 2015:\n",
    "                parquet_url_list.append(url)\n",
    "            if year == 2015:\n",
    "                month = int(date[5:7])\n",
    "                if month <= 6:\n",
    "                    parquet_url_list.append(url) \n",
    "    return parquet_url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294276b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"tpep_pickup_datetime\", \n",
    "           \"pickup_longitude\", \n",
    "           \"pickup_latitude\", \n",
    "           \"dropoff_longitude\", \n",
    "           \"dropoff_latitude\",\n",
    "           \"total_amount\"]\n",
    "columns2 = [\"pickup_datetime\", \n",
    "           \"pickup_longitude\", \n",
    "           \"pickup_latitude\", \n",
    "           \"dropoff_longitude\", \n",
    "           \"dropoff_latitude\",\n",
    "           \"total_amount\"]\n",
    "columns3 = [\"Trip_Pickup_DateTime\", \n",
    "           \"Start_Lon\",\n",
    "           \"Start_Lat\",\n",
    "           \"End_Lon\", \n",
    "           \"End_Lat\",\n",
    "           \"Total_Amt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lat_log_column(dataframe):\n",
    "    \n",
    "    print('add_lat_log_column')\n",
    "    dftaxi = gpd.read_file('taxi_zones.shp')\n",
    "    dftaxi = dftaxi.to_crs(CRS)\n",
    "    \n",
    "    lat1=[]\n",
    "    lon1=[]\n",
    "    lat2=[]\n",
    "    lon2=[]\n",
    "    for LocationID in dataframe[\"PULocationID\"]:\n",
    "        lat=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.x\n",
    "        lon=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.y\n",
    "        if lat.empty:\n",
    "            lat1.append(0)\n",
    "        else:\n",
    "            lat1.append(lat[0])\n",
    "        if lon.empty:\n",
    "            lon1.append(0)\n",
    "        else:\n",
    "            lon1.append(log[0])\n",
    "    \n",
    "    for LocationID in dataframe[\"DOLocationID\"]:\n",
    "        lat=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.x\n",
    "        lon=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.y\n",
    "        if lat.empty:\n",
    "            lat2.append(0)\n",
    "        else:\n",
    "            lat2.append(lat[0])\n",
    "        if log.empty:\n",
    "            lon2.append(0)\n",
    "        else:\n",
    "            lon2.append(lon[0])\n",
    "    dataframe['pickup_latitude']=lat1\n",
    "    dataframe['pickup_longitude']=lon1\n",
    "    dataframe['dropoff_latitude']=lat2\n",
    "    dataframe['dropoff_longitude']=lon2\n",
    "    dataframe.to_csv(\"2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url):\n",
    "    \n",
    "    reponse = requests.get(url)\n",
    "\n",
    "    filename=url.split('/')[-1]\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(reponse.content)\n",
    "    \n",
    "    df = pd.read_parquet(filename)\n",
    "    print(filename)\n",
    "    print(df.columns)\n",
    "    df = df.sample(n=sample_size,ignore_index=True)\n",
    "    try:\n",
    "        if \"PULocationID\" in df.columns:\n",
    "            add_latlog_column(df)\n",
    "        df = df[columns]\n",
    "    except:\n",
    "        try:\n",
    "            df = df[columns2]\n",
    "        except:\n",
    "            try:\n",
    "                df = df[columns3]\n",
    "            except:\n",
    "                add_latlog_column(df)\n",
    "                df = df[columns2]\n",
    "    df.columns = columns2\n",
    "    df = df.sample(n=sample_size,random_state = 1,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "\n",
    "    all_parquet_urls = find_taxi_parquet_urls()\n",
    "    \n",
    "    for parquet_url in all_parquet_urls:\n",
    "        \n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        taxi_data.to_csv(TAXI_CSV)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = find_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_DATA)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
