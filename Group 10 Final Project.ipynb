{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a606159",
   "metadata": {},
   "source": [
    "## Group 10 \n",
    "### Yixuan (Sharon) Qian - yq2348\n",
    "### Michelle Jingyi Zhou - jz3508"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.4.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.4.2) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.4.2) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.4.2) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.4.2) (1.16.0)\n",
      "Requirement already satisfied: pyarrow==7.0.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (7.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pyarrow==7.0.0) (1.21.5)\n",
      "Requirement already satisfied: geopandas in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (3.5.0)\n",
      "Requirement already satisfied: shapely>=1.7 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (2.0.1)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (1.4.2)\n",
      "Requirement already satisfied: packaging in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (21.3)\n",
      "Requirement already satisfied: fiona>=1.8 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (1.9.3)\n",
      "Requirement already satisfied: click~=8.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (8.0.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (21.4.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: certifi in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2021.10.8)\n",
      "Requirement already satisfied: importlib-metadata in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (4.11.3)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: munch>=2.3.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: six in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (1.21.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from packaging->geopandas) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# all import statements needed for the project\n",
    "\n",
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4 \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install pandas==1.4.2\n",
    "!pip install pyarrow==7.0.0\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import numpy as np\n",
    "import re\n",
    "import os.path\n",
    "import glob\n",
    "!pip install geopandas\n",
    "import geopandas\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants we might need\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV_FILES = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\",\n",
    "                    \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "\n",
    "EARTH_RADIUS = 6378.137\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"/content/queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "Overview: For Part 1, we downloaded the Parquet files, cleaned and filtered for the relevant data, filling in missing data, and generating samples of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance\n",
    "\n",
    "1.rad(d) function converts numeric degrees to radians\n",
    "\n",
    "2.distance calculation function\n",
    "calculate_distance_with_coords(from_coord, to_coord) calculates the distance btween coordinates\n",
    "\n",
    "3.add_distance_column(dataframe)\n",
    "\n",
    "Add a column call_distance to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a34ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts numeric degrees to radians\n",
    "# d is diameter\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbbe6cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T17:32:09.000495Z",
     "start_time": "2023-04-24T17:32:08.965159Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    \"\"\"\n",
    "     this function calculates the distance between two points\n",
    "     using their latitude and longitude coordinates\n",
    "     \n",
    "     Output: distance ----- distnace between two coordinates\n",
    "    \"\"\"\n",
    "    \n",
    "    rad_lat1 = rad(from_coord[0])\n",
    "    rad_lon1 = rad(from_coord[1])\n",
    "    rad_lat2 = rad(to_coord[0])\n",
    "    rad_lon2 = rad(to_coord[1])\n",
    "    \n",
    "    a = rad_lat1 - rad_lat2\n",
    "    b = rad_lon1 - rad_lon2\n",
    "    distance_radius = 2 * math.asin(\n",
    "        math.sqrt(math.pow(math.sin(a / 2), 2) \n",
    "                  + math.cos(rad_lat1) * math.cos(rad_lat2) * math.pow(math.sin(b / 2), 2)))\n",
    "    distance = distance_radius * EARTH_RADIUS\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df):\n",
    "    \"\"\"\n",
    "    Add a column â€˜cal_distance' to the dataframe 'sample_df'\n",
    "\n",
    "    \"\"\"\n",
    "    distance_list = []\n",
    "    for i in range(len(df)):\n",
    "        s_lat, s_lon  = df[\"pickup_latitude\"][i], df[\"pickup_longitude\"][i]\n",
    "        e_lat, e_lon  = df[\"dropoff_latitude\"][i], df[\"dropoff_longitude\"][i]\n",
    "        if s_lat and s_lon and e_lat and e_lon:\n",
    "            outside = True\n",
    "            if s_lat > NEW_YORK_BOX_COORDS[0][0] and s_lat < NEW_YORK_BOX_COORDS[1][0]:\n",
    "                if e_lat > NEW_YORK_BOX_COORDS[0][0] and e_lat < NEW_YORK_BOX_COORDS[1][0]:\n",
    "                    if s_lon > NEW_YORK_BOX_COORDS[0][1] and s_lon < NEW_YORK_BOX_COORDS[1][1]:\n",
    "                        if e_lon > NEW_YORK_BOX_COORDS[0][1] and e_lon < NEW_YORK_BOX_COORDS[1][1]:\n",
    "                            outside = False\n",
    "            if outside:\n",
    "                s_lat, s_lon, e_lat, e_lon = None, None, None, None\n",
    "        from_coord = (s_lat, s_lon)\n",
    "        to_coord = (e_lat, e_lon)\n",
    "        if s_lat and s_lon and e_lat and e_lon:\n",
    "            distance_list.append(calculate_distance_with_coords(from_coord, to_coord))\n",
    "        else:\n",
    "            distance_list.append(None)\n",
    "    df[\"cal_distance\"] = distance_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd328f1e",
   "metadata": {},
   "source": [
    "### Use Taxi Zones Shapefile to Convert to Coordinates\n",
    "\n",
    "1. get_latlon_from_locationID():\n",
    "\n",
    "We load taxi zones shapefile\n",
    "\n",
    "2. convert_id_to_latlon(sample_tables)\n",
    "\n",
    "we convert area ID column into  coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b1721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load taxi zones from a shapefile and add new columns.\n",
    "\n",
    "def get_latlon_from_locationID():\n",
    "    # Read the taxi zone shapefile and convert it to CRS\n",
    "    #gdf is GeoDataFrame\n",
    "    gdf = geopandas.read_file(\"/content/taxi_zones.shp\")\n",
    "    gdf = gdf.to_crs(CRS)\n",
    "    \n",
    "    # Get the lon and lat of the centroid of each  zone\n",
    "    lon = gdf.centroid.x\n",
    "    lat = gdf.centroid.y\n",
    "    \n",
    "    # Add new columns to the dataframe to store the lon and lat\n",
    "    gdf[\"lon\"] = lon\n",
    "    gdf[\"lat\"] = lat\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16056eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id_to_latlon(samples_df):\n",
    "    \"\"\"\n",
    "    Convert area ID column into two coordinates\n",
    "    \"\"\"\n",
    "    gdf = get_latlon_from_locationID()\n",
    "\n",
    "    def get_coords(location_id):\n",
    "        if location_id < 264 and location_id in gdf[\"LocationID\"].values:\n",
    "            row_index = gdf[gdf[\"LocationID\"] == location_id].index.values[0]\n",
    "            lon, lat = float(gdf[\"lon\"][row_index]), float(gdf[\"lat\"][row_index])\n",
    "\n",
    "            if (\n",
    "                NEW_YORK_BOX_COORDS[0][0] < lat < NEW_YORK_BOX_COORDS[1][0]\n",
    "                and NEW_YORK_BOX_COORDS[0][1] < lon < NEW_YORK_BOX_COORDS[1][1]\n",
    "            ):\n",
    "                return lon, lat\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    start_coords = samples_df[\"PULocationID\"].apply(get_coords)\n",
    "    end_coords = samples_df[\"DOLocationID\"].apply(get_coords)\n",
    "\n",
    "    samples_df[\"pickup_longitude\"] = [coord[0] for coord in start_coords]\n",
    "    samples_df[\"pickup_latitude\"] = [coord[1] for coord in start_coords]\n",
    "    samples_df[\"dropoff_longitude\"] = [coord[0] for coord in end_coords]\n",
    "    samples_df[\"dropoff_latitude\"] = [coord[1] for coord in end_coords]\n",
    "    return samples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data\n",
    "\n",
    "1. this function programmatically downloads the Yellow Taxi Parquet files for a specific date range 2009-01 and 2015-06 from the website. it returns a list that contains all taxi data url in TAXI_URL \"\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "3. we added latitude and logitude from taxi_zones. We also added pickup_latitude, dropoff_latitude, dropoff_latitude and dropoff_longitude as columns to the dataframe for convenient calculation\n",
    "\n",
    "4. Download Parquet files, get some sample from these files, Clean the dataframe according to existing location IDs, Write data into .csv and return dataframe\n",
    "\n",
    "5. get_and_clean_taxi_data\n",
    "Get taxi data. If taxi.csv exists, read-only. Otherwise, download data and generate taxi.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_parquet_urls():\n",
    "\n",
    "    parquet_urls = []\n",
    "    response = requests.get(url=TAXI_URL)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            url = link['href']\n",
    "            if 'yellow_tripdata' in url and '.parquet' in url:\n",
    "                date_str = url.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "                year = int(date_str[:4])\n",
    "                month = int(date_str[4:6])\n",
    "                if year < 2015 or (year == 2015 and month <= 6):\n",
    "                    parquet_urls.append(url)\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5509ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datetime(samples_df):\n",
    "    \"\"\"\n",
    "    Change column type for date column and add columns for specific time data\n",
    "\n",
    "    Arguments:\n",
    "    df -- a dataframe with time data column\n",
    "\n",
    "    \"\"\"\n",
    "    if \"tpep_pickup_datetime\" in samples_df.columns:\n",
    "        samples_df['tpep_pickup_datetime'] = pd.to_datetime(samples_df['tpep_pickup_datetime'])\n",
    "        samples_df['tpep_dropoff_datetime'] = pd.to_datetime(samples_df['tpep_dropoff_datetime'])\n",
    "\n",
    "        datetime_columns = {\n",
    "            'DATE': samples_df['tpep_pickup_datetime'],\n",
    "            'YEAR': samples_df['tpep_pickup_datetime'].dt.year.astype(int),\n",
    "            'MONTH': samples_df['tpep_pickup_datetime'].dt.month.astype(int),\n",
    "            'DAY': samples_df['tpep_pickup_datetime'].dt.day.astype(int),\n",
    "            'HOUR': samples_df['tpep_pickup_datetime'].dt.hour.astype(int),\n",
    "            'WEEK': samples_df['tpep_pickup_datetime'].dt.dayofweek + 1 \n",
    "        }\n",
    "    else:\n",
    "        datetime_columns = {\n",
    "            'tpep_pickup_datetime': None,\n",
    "            'YEAR': None,\n",
    "            'MONTH': None,\n",
    "            'DAY': None,\n",
    "            'HOUR': None,\n",
    "            'WEEK': None\n",
    "        } \n",
    "    samples_df = samples_df.assign(**datetime_columns) \n",
    "    return samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d9aab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    \"\"\"\n",
    "    Download, read, sample, and clean one month of taxi data\n",
    "\n",
    "    Arguments:\n",
    "    url -- a url for downloading a specific month's taxi parquet file\n",
    "\n",
    "    Returns:\n",
    "    cleaned_data -- a DataFrame containing cleaned taxi data for one month\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "    columns_by_year = {\n",
    "        '2011_2015': [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"pickup_longitude\",\n",
    "                  \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\"],\n",
    "        '2010': [\"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "               \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\"],\n",
    "        '2009': [\"Trip_Pickup_DateTime\", \"Trip_Dropoff_DateTime\", \"Passenger_Count\", \"Trip_Distance\", \"Start_Lon\", \"Start_Lat\",\n",
    "               \"End_Lon\", \"End_Lat\", \"Tip_Amt\"]\n",
    "    }\n",
    "    \n",
    "    print('read data from ', file_name)\n",
    "    raw_data = pd.read_parquet(file_name)\n",
    "    # to speed up the data clean process\n",
    "    raw_data = raw_data.sample(4000)\n",
    "    raw_data.reset_index(inplace=True)\n",
    "\n",
    "    year_key = '2011_2015' if not re.search(r\"2009|2010\", file_name) else '2010' if re.search(r\"2010\", file_name) else '2009'\n",
    "\n",
    "    if year_key == '2011_2015':\n",
    "        raw_data = convert_id_to_latlon(raw_data)\n",
    "\n",
    "    cleaned_data = raw_data[columns_by_year[year_key]]\n",
    "    unified_column_names = {columns_by_year[year_key][i]: columns_by_year['2011_2015'][i] for i in range(len(columns_by_year[year_key]))}\n",
    "    cleaned_data.rename(columns=unified_column_names, inplace=True)\n",
    "\n",
    "    cleaned_data[columns_by_year['2011_2015'][:8]] = cleaned_data[columns_by_year['2011_2015'][:8]].replace(0.0, None)\n",
    "    cleaned_data.dropna(inplace=True)\n",
    "    \n",
    "    #generate a sampling of Taxi data that's roughly equal to the Uber dataset\n",
    "    #Uber, 200,000/78=2564. \n",
    "    cleaned_data = cleaned_data.sample(2564)\n",
    "    cleaned_data.reset_index(inplace=True)\n",
    "\n",
    "    cleaned_data = process_datetime(cleaned_data)\n",
    "    cleaned_data = add_distance_column(cleaned_data)\n",
    "    cleaned_data.drop([\"index\"], axis=1, inplace=True)\n",
    "\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63979eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = get_taxi_parquet_urls()\n",
    "    # print(all_parquet_urls)\n",
    "    for parquet_url in all_parquet_urls:\n",
    "        \n",
    "        dataframe = get_and_clean_month_taxi_data(parquet_url)\n",
    "                \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "    \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200776ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_and_clean_taxi_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     taxi_data \u001b[38;5;241m=\u001b[39m taxi_data\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;241m~\u001b[39mtaxi_data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^Unnamed\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     taxi_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_and_clean_taxi_data\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_and_clean_taxi_data' is not defined"
     ]
    }
   ],
   "source": [
    "if os.path.exists('/content/taxi_clean_data.csv'): \n",
    "    taxi_data = pd.read_csv('/content/taxi_clean_data.csv')\n",
    "    taxi_data = taxi_data.loc[:, ~taxi_data.columns.str.contains('^Unnamed')]\n",
    "else:\n",
    "    taxi_data = get_and_clean_taxi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "1. load_and_clean_uber_data\n",
    "\n",
    "this function load data from Uber csv file, and add columns to the pd df \n",
    "and Load data from input file and add date columns to it\n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"\n",
    "    Load data from input file and add date columns to it  \n",
    "    \n",
    "    Arguments:\n",
    "    csv_file -- file name containing the data\n",
    "    \n",
    "    Returns:\n",
    "    pd_data -- a dataframe that contains cleaned data from input file\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load data from Uber CSV file\n",
    "    pd_df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Convert pickup datetime column to pd datetime format\n",
    "    pd_df['pickup_datetime'] = pd.to_datetime(pd_df['pickup_datetime'])\n",
    "    \n",
    "    # get year, month, week, day, hour from pickup datetime column\n",
    "    pd_df['YEAR'] = pd_df['pickup_datetime'].dt.year.astype(int)\n",
    "    pd_df['MONTH'] = pd_df['pickup_datetime'].dt.month.astype(int)\n",
    "    pd_df['WEEK'] = pd_df['pickup_datetime'].dt.dayofweek + 1\n",
    "    pd_df['DAY'] = pd_df['pickup_datetime'].dt.day.astype(int)\n",
    "    pd_df['HOUR'] = pd_df['pickup_datetime'].dt.hour.astype(int)\n",
    "    \n",
    "    pd_df = pd_df.reset_index(drop=True)\n",
    "    \n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    \"\"\"\n",
    "    Load and clean Uber data, and add a column for distance in miles\n",
    "    \n",
    "    Returns:\n",
    "    uber_dataframe -- a dataframe containing cleaned and processed Uber data\n",
    "    \n",
    "    \"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    \n",
    "    valid_columns = [\"fare_amount\", \"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "                     \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "    \n",
    "    # put 0.0 values in valid columns with NaN\n",
    "    uber_dataframe[valid_columns] = uber_dataframe[valid_columns].replace(0.0, None)\n",
    "    \n",
    "    # Drop rows with NaN values in valid columns\n",
    "    uber_dataframe.dropna(subset=valid_columns, inplace=True)\n",
    "    uber_dataframe = uber_dataframe.reset_index(drop=True)\n",
    "    \n",
    "    # Add a new column for the distance traveled during each Uber ride\n",
    "    add_distance_column(uber_dataframe)\n",
    "    \n",
    "    valid_columns.append(\"cal_distance\")\n",
    "    valid_columns.remove(\"fare_amount\")\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    uber_dataframe.dropna(subset=valid_columns, inplace=True)\n",
    "    uber_dataframe = uber_dataframe.drop([\"key\", \"fare_amount\"], axis=1)\n",
    "\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e566081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (7.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pyarrow) (1.21.5)\n",
      "Requirement already satisfied: fastparquet in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (2023.2.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: packaging in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (1.21.5)\n",
      "Collecting pandas>=1.5.0\n",
      "  Using cached pandas-2.0.1-cp39-cp39-macosx_10_9_x86_64.whl (11.8 MB)\n",
      "Requirement already satisfied: fsspec in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (2022.2.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from packaging->fastparquet) (3.0.4)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.2\n",
      "    Uninstalling pandas-1.4.2:\n",
      "      Successfully uninstalled pandas-1.4.2\n",
      "Successfully installed pandas-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "339997e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>cal_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24238194</td>\n",
       "      <td>2015-05-07 19:52:06+00:00</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>1.685209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27835199</td>\n",
       "      <td>2009-07-17 20:04:56+00:00</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.99471</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>2.460343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44984355</td>\n",
       "      <td>2009-08-24 21:45:00+00:00</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.74077</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>5.042019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25894730</td>\n",
       "      <td>2009-06-26 08:22:21+00:00</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>1.663545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17610152</td>\n",
       "      <td>2014-08-28 17:47:00+00:00</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>4.480464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           pickup_datetime pickup_longitude pickup_latitude  \\\n",
       "0    24238194 2015-05-07 19:52:06+00:00       -73.999817       40.738354   \n",
       "1    27835199 2009-07-17 20:04:56+00:00       -73.994355       40.728225   \n",
       "2    44984355 2009-08-24 21:45:00+00:00       -74.005043        40.74077   \n",
       "3    25894730 2009-06-26 08:22:21+00:00       -73.976124       40.790844   \n",
       "4    17610152 2014-08-28 17:47:00+00:00       -73.925023       40.744085   \n",
       "\n",
       "  dropoff_longitude dropoff_latitude  passenger_count  YEAR  MONTH  WEEK  DAY  \\\n",
       "0        -73.999512        40.723217                1  2015      5     4    7   \n",
       "1         -73.99471        40.750325                1  2009      7     5   17   \n",
       "2        -73.962565        40.772647                1  2009      8     1   24   \n",
       "3        -73.965316        40.803349                3  2009      6     5   26   \n",
       "4        -73.973082        40.761247                5  2014      8     4   28   \n",
       "\n",
       "   HOUR  cal_distance  \n",
       "0    19      1.685209  \n",
       "1    20      2.460343  \n",
       "2    21      5.042019  \n",
       "3     8      1.663545  \n",
       "4    17      4.480464  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data = get_uber_data()\n",
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "1. function get_all_weather_csvs(directory)\n",
    "\n",
    "\n",
    "\n",
    "2. function clean_month_weather_data_hourly(csv_file):\n",
    "\n",
    "\n",
    "3. function clean_month_weather_data_daily(csv_file):\n",
    "\n",
    "\n",
    "4. function load_and_clean_weather_data():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    csv_files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\") and \"weather\" in file.lower():\n",
    "            csv_files.append(os.path.join(directory, file))\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "\n",
    "    columns_to_keep = [\"DATE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]\n",
    "    weather_data = pd.read_csv(csv_file, usecols=columns_to_keep)\n",
    "\n",
    "    # Replace missing values in HourlyPrecipitation with 0.0\n",
    "    weather_data[\"HourlyPrecipitation\"] = weather_data[\"HourlyPrecipitation\"].fillna(0)\n",
    "\n",
    "    # Convert DATE column to datetime and extract year, month, day, and hour\n",
    "    weather_data[\"DATE\"] = pd.to_datetime(weather_data[\"DATE\"])\n",
    "    weather_data[\"YEAR\"] = weather_data[\"DATE\"].dt.year.astype(int)\n",
    "    weather_data[\"MONTH\"] = weather_data[\"DATE\"].dt.month.astype(int)\n",
    "    weather_data[\"WEEK\"] = weather_data[\"DATE\"].dt.dayofweek + 1\n",
    "    weather_data[\"DAY\"] = weather_data[\"DATE\"].dt.day.astype(int)\n",
    "    weather_data[\"HOUR\"] = weather_data[\"DATE\"].dt.hour.astype(int)\n",
    "\n",
    "\n",
    "    # gethourly weather data for each day\n",
    "    hourly_weather = []\n",
    "    hourly_weather_columns = list(weather_data.columns)\n",
    "    date_string = \"\"\n",
    "    for i in range(weather_data.shape[0]):\n",
    "        tmp_date_string = str(weather_data.iloc[i, :][\"YEAR\"]) + str(weather_data.iloc[i, :][\"MONTH\"]) + str(weather_data.iloc[i, :][\"DAY\"]) + str(weather_data.iloc[i, :][\"HOUR\"])\n",
    "        if tmp_date_string == date_string:\n",
    "            continue\n",
    "        else:\n",
    "            hourly_weather.append(weather_data.iloc[i, :].to_list())\n",
    "            date_string = tmp_date_string\n",
    "\n",
    "    # Create a new df from the hourly weather data and ignore any rows with no values\n",
    "    hourly_weather_data = pd.DataFrame(hourly_weather, columns=hourly_weather_columns)\n",
    "    hourly_weather_data.dropna(inplace=True)\n",
    "\n",
    "    return hourly_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \n",
    "    columns_to_keep = [\"DATE\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\", \"REPORT_TYPE\"]\n",
    "    \n",
    "    weather_data = pd.read_csv(csv_file, usecols=columns_to_keep)\n",
    "    weather_data.dropna(subset=[\"DailyAverageWindSpeed\"], inplace=True)\n",
    "    weather_data[[\"DailyPrecipitation\"]] = weather_data[[\"DailyPrecipitation\"]].fillna(0)\n",
    "    \n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "    weather_data['YEAR'] = weather_data['DATE'].dt.year.astype(int)\n",
    "    weather_data['MONTH'] = weather_data['DATE'].dt.month.astype(int)\n",
    "    weather_data[\"WEEK\"] = weather_data['DATE'].dt.dayofweek + 1\n",
    "    weather_data['DAY'] = weather_data['DATE'].dt.day.astype(int)\n",
    "\n",
    "    \n",
    "    date_str = \"\"\n",
    "    daily_weather = []\n",
    "    daily_weather_columns = list(weather_data.columns)\n",
    "    for i in range(weather_data.shape[0]):\n",
    "        tmp_date_str = str(weather_data.iloc[i,:][\"YEAR\"]) + str(weather_data.iloc[i,:][\"MONTH\"]) + str(weather_data.iloc[i,:][\"DAY\"])\n",
    "        if tmp_date_str == date_str:\n",
    "            continue\n",
    "        else:\n",
    "            daily_weather.append(weather_data.iloc[i,:].to_list())\n",
    "            date_str = tmp_date_str \n",
    "    \n",
    "    daily_weather_data = pd.DataFrame(daily_weather, columns=daily_weather_columns)\n",
    "    \n",
    "    return daily_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in WEATHER_CSV_FILES:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 03:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 04:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 DATE HourlyPrecipitation  HourlyWindSpeed  YEAR  MONTH  WEEK  \\\n",
       "0 2009-01-01 00:51:00                   0             18.0  2009      1     4   \n",
       "1 2009-01-01 01:51:00                   0             18.0  2009      1     4   \n",
       "2 2009-01-01 02:51:00                   0             18.0  2009      1     4   \n",
       "3 2009-01-01 03:51:00                   0              8.0  2009      1     4   \n",
       "4 2009-01-01 04:51:00                   0             11.0  2009      1     4   \n",
       "\n",
       "   DAY  HOUR  \n",
       "0    1     0  \n",
       "1    1     1  \n",
       "2    1     2  \n",
       "3    1     3  \n",
       "4    1     4  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>REPORT_TYPE</th>\n",
       "      <th>DailyAverageWindSpeed</th>\n",
       "      <th>DailyPrecipitation</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-31 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-08-01 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-08-02 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-08-03 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-08-04 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DATE REPORT_TYPE DailyAverageWindSpeed DailyPrecipitation  \\\n",
       "0  2012-07-31 23:59:00       SOD                     3.8               0.00   \n",
       "1  2012-08-01 23:59:00       SOD                     2.3               0.64   \n",
       "2  2012-08-02 23:59:00       SOD                     2.7               0.00   \n",
       "3  2012-08-03 23:59:00       SOD                     3.5               0.00   \n",
       "4  2012-08-04 23:59:00       SOD                     3.1               0.00   \n",
       "\n",
       "   YEAR MONTH WEEK DAY  \n",
       "0  2012     7    2  31  \n",
       "1  2012     8    3   1  \n",
       "2  2012     8    4   2  \n",
       "3  2012     8    5   3  \n",
       "4  2012     8    6   4  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "create table if not exists hourly_weather(\n",
    "  hid int primary key, \n",
    "  date timestamp, \n",
    "  year int, \n",
    "  month int, \n",
    "  week int, \n",
    "  hour int, \n",
    "  hourlyPrecipitation float, \n",
    "  hourlyWindSpeed float\n",
    ")\n",
    "\"\"\"\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "create table if not exists daily_weather(\n",
    "  did int primary key,\n",
    "  date timestamp,\n",
    "  dailyPrecipitation float,\n",
    "  dailyAverageWindSpeed float,\n",
    "  year int,\n",
    "  month int,\n",
    "  day int, \n",
    "  week int \n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "create table if not exists taxi_trip(\n",
    "  tid int primary key,\n",
    "  date timestamp,\n",
    "  tpep_pickup_datetime timestamp,\n",
    "  tpep_dropoff_datetime timestamp,\n",
    "  trip_distance float,\n",
    "  tip_amount float,\n",
    "  passenger_count int,\n",
    "  pickup_longitude float,\n",
    "  pickup_latitude float,\n",
    "  dropoff_longitude float,\n",
    "  dropoff_latitude float,\n",
    "  cal_distance float,\n",
    "  year int,\n",
    "  month int,\n",
    "  day int,\n",
    "  hour int,\n",
    "  week int\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "create table if not exists uber_trip(\n",
    "  uid int primary key,\n",
    "  pickup_datetime timestamp,\n",
    "  pickup_longitude float,\n",
    "  pickup_latitude float,\n",
    "  dropoff_longitude float,\n",
    "  dropoff_latitude float,\n",
    "  passenger_count int,\n",
    "  year int,\n",
    "  month int,\n",
    "  day int,\n",
    "  week int,\n",
    "  hour int,\n",
    "  cal_distance float\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x7fc48a54d910>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the tables with the schema files\n",
    "from sqlalchemy import text\n",
    "conn = engine.connect()\n",
    "conn.execute(text('drop table if exists taxi_trip'))\n",
    "conn.execute(text('drop table if exists uber_trip'))\n",
    "conn.execute(text('drop table if exists hourly_weather'))\n",
    "conn.execute(text('drop table if exists daily_weather'))\n",
    "conn.execute(text(HOURLY_WEATHER_SCHEMA))\n",
    "conn.execute(text(DAILY_WEATHER_SCHEMA))\n",
    "conn.execute(text(TAXI_TRIPS_SCHEMA))\n",
    "conn.execute(text(UBER_TRIPS_SCHEMA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for table in table_to_df_dict.keys():\n",
    "        table_to_df_dict[table].to_sql(table, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taxi_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m map_table_name_to_dataframe \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaxi_trips\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtaxi_data\u001b[49m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muber_trips\u001b[39m\u001b[38;5;124m\"\u001b[39m: uber_data,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhourly_weather\u001b[39m\u001b[38;5;124m\"\u001b[39m: hourly_data,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdaily_weather\u001b[39m\u001b[38;5;124m\"\u001b[39m: daily_data,\n\u001b[1;32m      6\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taxi_data' is not defined"
     ]
    }
   ],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74004f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'map_table_name_to_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m write_dataframes_to_table(\u001b[43mmap_table_name_to_dataframe\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'map_table_name_to_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(f'{QUERY_DIRECTORY}/{outfile}', 'w') as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "For 01-2009 through 06-2015, show the popularity of Yellow Taxi rides for each hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) near \",\": syntax error\n[SQL: \nSELECT, COUNT(*) FROM taxi_trip\nWHERE tpep_pickup_datetime >= '2009-01-01' and tpep_pickup_datetime <= '2015-06-30'\nGROUP BY HOUR\nORDER BY count(*) desc\n]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1808\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1808\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 732\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: near \",\": syntax error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m QUERY_1_FILENAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery1.sql\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m QUERY_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mSELECT, COUNT(*) FROM taxi_trip\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mWHERE tpep_pickup_datetime >= \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2009-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and tpep_pickup_datetime <= \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2015-06-30\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mGROUP BY HOUR\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mORDER BY count(*) desc\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m query1_result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUERY_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m     10\u001b[0m query1_result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1295\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[1;32m   1292\u001b[0m         exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement), replace_context\u001b[38;5;241m=\u001b[39merr\n\u001b[1;32m   1293\u001b[0m     )\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_EMPTY_EXECUTION_OPTS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/sql/elements.py:325\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, multiparams, params, execution_options, _force)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_on_connection\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m, connection, multiparams, params, execution_options, _force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    323\u001b[0m ):\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _force \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_execution:\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1487\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, multiparams, params, execution_options)\u001b[0m\n\u001b[1;32m   1475\u001b[0m compiled_cache \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[1;32m   1477\u001b[0m )\n\u001b[1;32m   1479\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1480\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[1;32m   1481\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1486\u001b[0m )\n\u001b[0;32m-> 1487\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1501\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1502\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         ret,\n\u001b[1;32m   1507\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1851\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             branched\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1851\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:2032\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   2030\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(newraise, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m-> 2032\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m        \u001b[49m\u001b[43msqlalchemy_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_traceback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2036\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(exc_info[\u001b[38;5;241m1\u001b[39m], with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/util/compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    204\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1808\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1806\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1808\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1814\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1815\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1819\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1820\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 732\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) near \",\": syntax error\n[SQL: \nSELECT, COUNT(*) FROM taxi_trip\nWHERE tpep_pickup_datetime >= '2009-01-01' and tpep_pickup_datetime <= '2015-06-30'\nGROUP BY HOUR\nORDER BY count(*) desc\n]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)"
     ]
    }
   ],
   "source": [
    "QUERY_1_FILENAME = \"query1.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT, COUNT(*) FROM taxi_trip\n",
    "WHERE tpep_pickup_datetime >= '2009-01-01' and tpep_pickup_datetime <= '2015-06-30'\n",
    "GROUP BY HOUR\n",
    "ORDER BY count(*) desc\n",
    "\"\"\"\n",
    "query1_result = conn.execute(text(QUERY_1)).fetchall()\n",
    "query1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950daa00",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "For the same time frame, show the popularity of Uber rides for each day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31b1cb49",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such table: uber_trips\n[SQL: \nSELECT WEEK, COUNT(*) from uber_trips\nWHERE pickup_datetime >= '2009-01-01' and tpep_pickup_datetime <= '2015-06-30'\nGROUP BY WEEK \nORDER BY count DESC \n]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1808\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1808\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 732\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: uber_trips",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m QUERY_2_FILENAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery2.sql\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m QUERY_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mSELECT WEEK, COUNT(*) from uber_trips\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mWHERE pickup_datetime >= \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2009-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and tpep_pickup_datetime <= \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2015-06-30\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mGROUP BY WEEK \u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mORDER BY count DESC \u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m query2_result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUERY_2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m     10\u001b[0m query2_result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1295\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[1;32m   1292\u001b[0m         exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement), replace_context\u001b[38;5;241m=\u001b[39merr\n\u001b[1;32m   1293\u001b[0m     )\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_EMPTY_EXECUTION_OPTS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/sql/elements.py:325\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, multiparams, params, execution_options, _force)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_on_connection\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m, connection, multiparams, params, execution_options, _force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    323\u001b[0m ):\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _force \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_execution:\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1487\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, multiparams, params, execution_options)\u001b[0m\n\u001b[1;32m   1475\u001b[0m compiled_cache \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[1;32m   1477\u001b[0m )\n\u001b[1;32m   1479\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1480\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[1;32m   1481\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1486\u001b[0m )\n\u001b[0;32m-> 1487\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1501\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1502\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         ret,\n\u001b[1;32m   1507\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1851\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             branched\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1851\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:2032\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   2030\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(newraise, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m-> 2032\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m        \u001b[49m\u001b[43msqlalchemy_exception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_traceback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2036\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(exc_info[\u001b[38;5;241m1\u001b[39m], with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/util/compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    204\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:1808\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1806\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1808\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1814\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1815\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1819\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1820\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/default.py:732\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 732\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: uber_trips\n[SQL: \nSELECT WEEK, COUNT(*) from uber_trips\nWHERE pickup_datetime >= '2009-01-01' and tpep_pickup_datetime <= '2015-06-30'\nGROUP BY WEEK \nORDER BY count DESC \n]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)"
     ]
    }
   ],
   "source": [
    "QUERY_2_FILENAME = \"query2.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT WEEK, COUNT(*) from uber_trips\n",
    "WHERE pickup_datetime >= '2009-01-01' and tpep_pickup_datetime <= '2015-06-30'\n",
    "GROUP BY WEEK \n",
    "ORDER BY count DESC \n",
    "\"\"\"\n",
    "query2_result = conn.execute(text(QUERY_2)).fetchall()\n",
    "query2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a2112a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
