{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a606159",
   "metadata": {},
   "source": [
    "## Group 10 \n",
    "### Yixuan (Sharon) Qian - yq2348\n",
    "### Michelle Jingyi Zhou - jz3508"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import bs4 \n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import numpy as np\n",
    "import re\n",
    "import os.path\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants we might need\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV_FILES = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\",\n",
    "                    \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "\n",
    "EARTH_RADIUS = 6378.137\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "Overview: For Part 1, we downloaded the Parquet files, cleaned and filtered for the relevant data, filling in missing data, and generating samples of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance\n",
    "\n",
    "1.rad(d) function converts numeric degrees to radians\n",
    "\n",
    "2.distance calculation function\n",
    "calculate_distance_with_coords(from_coord, to_coord) calculates the distance btween coordinates\n",
    "\n",
    "3.add_distance_column(dataframe)\n",
    "\n",
    "Add a column call_distance to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a5a34ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts numeric degrees to radians\n",
    "# d is diameter\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    rad_lat1 = rad(from_coord['pickup_latitude'])\n",
    "    rad_lon1 = rad(from_coord['pickup_longitude'])\n",
    "    rad_lat2 = rad(to_coord['dropoff_longitude'])\n",
    "    rad_lon2 = rad(to_coord['dropoff_latitude'])\n",
    "    a = rad_lat1 - rad_lat2\n",
    "    b = rad_lon1 - rad_lon2\n",
    "    distance_radius = 2 * math.asin(\n",
    "        math.sqrt(math.pow(math.sin(a / 2), 2)+ math.cos(rad_lat1) * math.cos(rad_lat2) * math.pow(math.sin(b / 2), 2)))\n",
    "    distance = distance_radius * EARTH_RADIUS\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df):\n",
    "    \"\"\"\n",
    "    Add a column with name 'cal_distance' to the dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    # Filter rows with valid coordinates within the NY box\n",
    "    valid_rows = (\n",
    "        (df[\"pickup_latitude\"] > NEW_YORK_BOX_COORDS[0][0])\n",
    "        & (df[\"pickup_latitude\"] < NEW_YORK_BOX_COORDS[1][0])\n",
    "        & (df[\"dropoff_latitude\"] > NEW_YORK_BOX_COORDS[0][0])\n",
    "        & (df[\"dropoff_latitude\"] < NEW_YORK_BOX_COORDS[1][0])\n",
    "        & (df[\"pickup_longitude\"] > NEW_YORK_BOX_COORDS[0][1])\n",
    "        & (df[\"pickup_longitude\"] < NEW_YORK_BOX_COORDS[1][1])\n",
    "        & (df[\"dropoff_longitude\"] > NEW_YORK_BOX_COORDS[0][1])\n",
    "        & (df[\"dropoff_longitude\"] < NEW_YORK_BOX_COORDS[1][1])\n",
    "    )\n",
    "    # Create new dataframe with valid rows we filtered\n",
    "    valid_df = df[valid_rows].copy()\n",
    "    \n",
    "    # Get pickup & dropoff coordinates in valid rows\n",
    "    from_coord = valid_df[['pickup_latitude', 'pickup_longitude']]\n",
    "    to_coord = valid_df[['dropoff_latitude', 'dropoff_longitude']]\n",
    "    \n",
    "    # Calculate distance for valid rows\n",
    "    valid_df['cal_distance'] = calculate_distance_with_coords(from_coord, to_coord)\n",
    "    \n",
    "    # Merge valid_df back into the original dataframe\n",
    "    final_df = df.merge(valid_df[['cal_distance']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd328f1e",
   "metadata": {},
   "source": [
    "### Use Taxi Zones Shapefile to Convert to Coordinates\n",
    "\n",
    "1. get_latlon_from_locationID():\n",
    "\n",
    "We load taxi zones shapefile\n",
    "\n",
    "2. convert_id_to_latlon(sample_tables)\n",
    "\n",
    "we convert area ID column into  coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61b1721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load taxi zones from a shapefile and add new columns.\n",
    "\n",
    "def get_latlon_from_locationID():\n",
    "    # Read the taxi zone shapefile and convert it to CRS\n",
    "    gdf = geopandas.read_file(\"taxi_zones.shp\")\n",
    "    gdf = gdf.to_crs(CRS)\n",
    "    \n",
    "    # Get the lon and lat of the centroid of each  zone\n",
    "    lon = gdf.centroid.x\n",
    "    lat = gdf.centroid.y\n",
    "    \n",
    "    # Add new columns to the dataframe to store the lon and lat\n",
    "    gdf[\"lon\"] = lon\n",
    "    gdf[\"lat\"] = lat\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "16056eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id_to_latlon(sample_tables):\n",
    "    \"\"\"\n",
    "    Convert area ID column into two coordinates\n",
    "    \"\"\"\n",
    "    gdf = get_latlon_from_locationID()\n",
    "\n",
    "    def get_coords(location_id):\n",
    "        if location_id < 264 and location_id in gdf[\"LocationID\"].values:\n",
    "            row = gdf[gdf[\"LocationID\"] == location_id][0]\n",
    "            lon, lat = row[\"lon\"], row[\"lat\"]\n",
    "\n",
    "            if (\n",
    "                NEW_YORK_BOX_COORDS[0][0] < lat < NEW_YORK_BOX_COORDS[1][0]\n",
    "                and NEW_YORK_BOX_COORDS[0][1] < lon < NEW_YORK_BOX_COORDS[1][1]\n",
    "            ):\n",
    "                return lon, lat\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    start_coords = samples_df[\"PULocationID\"].apply(get_coords)\n",
    "    end_coords = samples_df[\"DOLocationID\"].apply(get_coords)\n",
    "\n",
    "    samples_df[\"pickup_longitude\"] = [coord[0] for coord in start_coords]\n",
    "    samples_df[\"pickup_latitude\"] = [coord[1] for coord in start_coords]\n",
    "    samples_df[\"dropoff_longitude\"] = [coord[0] for coord in end_coords]\n",
    "    samples_df[\"dropoff_latitude\"] = [coord[1] for coord in end_coords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data\n",
    "\n",
    "1. this function programmatically downloads the Yellow Taxi Parquet files for a specific date range 2009-01 and 2015-06 from the website. it returns a list that contains all taxi data url in TAXI_URL \"\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "3. we added latitude and logitude from taxi_zones. We also added pickup_latitude, dropoff_latitude, dropoff_latitude and dropoff_longitude as columns to the dataframe for convenient calculation\n",
    "\n",
    "4. Download Parquet files, get some sample from these files, Clean the dataframe according to existing location IDs, Write data into .csv and return dataframe\n",
    "\n",
    "5. get_and_clean_taxi_data\n",
    "Get taxi data. If taxi.csv exists, read-only. Otherwise, download data and generate taxi.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_parquet_urls():\n",
    "\n",
    "    parquet_urls = []\n",
    "    response = requests.get(url=TAXI_URL)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            url = link['href']\n",
    "            if 'yellow_tripdata' in url and '.parquet' in url:\n",
    "                date_str = url.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "                year = int(date_str[:4])\n",
    "                month = int(date_str[4:6])\n",
    "                if year < 2015 or (year == 2015 and month <= 6):\n",
    "                    parquet_urls.append(url)\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5509ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datetime(df):\n",
    "    \"\"\"\n",
    "    Change column type for date column and add columns for specific time data\n",
    "\n",
    "    Arguments:\n",
    "    df -- a dataframe with time data column\n",
    "\n",
    "    \"\"\"\n",
    "    if \"tpep_pickup_datetime\" in df.columns:\n",
    "        samples_df['tpep_pickup_datetime'] = pd.to_datetime(samples_df['tpep_pickup_datetime'])\n",
    "        samples_df['tpep_dropoff_datetime'] = pd.to_datetime(samples_df['tpep_dropoff_datetime'])\n",
    "\n",
    "        datetime_columns = {\n",
    "            'DATE': samples_df['tpep_pickup_datetime'],\n",
    "            'YEAR': samples_df['tpep_pickup_datetime'].dt.year.astype(int),\n",
    "            'MONTH': samples_df['tpep_pickup_datetime'].dt.month.astype(int),\n",
    "            'DAY': samples_df['tpep_pickup_datetime'].dt.day.astype(int),\n",
    "            'HOUR': samples_df['tpep_pickup_datetime'].dt.hour.astype(int),\n",
    "            'WEEK': samples_df['tpep_pickup_datetime'].dt.dayofweek + 1  # 0-6 to 1-7\n",
    "        }\n",
    "    else:\n",
    "        datetime_columns = {\n",
    "            'tpep_pickup_datetime': None,\n",
    "            'YEAR': None,\n",
    "            'MONTH': None,\n",
    "            'DAY': None,\n",
    "            'HOUR': None,\n",
    "            'WEEK': None\n",
    "        }\n",
    "\n",
    "    samples_df = samples_df.assign(**datetime_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9d9aab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    \"\"\"\n",
    "    Download, read, sample, and clean one month of taxi data\n",
    "\n",
    "    Arguments:\n",
    "    url -- a url for downloading a specific month's taxi parquet file\n",
    "\n",
    "    Returns:\n",
    "    cleaned_data -- a DataFrame containing cleaned taxi data for one month\n",
    "    \"\"\"\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "    columns_by_year = {\n",
    "        '2011_2015': [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"pickup_longitude\",\n",
    "                  \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\"],\n",
    "        '2010': [\"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "               \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\"],\n",
    "        '2009': [\"Trip_Pickup_DateTime\", \"Trip_Dropoff_DateTime\", \"Passenger_Count\", \"Trip_Distance\", \"Start_Lon\", \"Start_Lat\",\n",
    "               \"End_Lon\", \"End_Lat\", \"Tip_Amt\"]\n",
    "    }\n",
    "\n",
    "    raw_data = pd.read_parquet(file_name)\n",
    "    raw_data = raw_data.sample(20000)\n",
    "    raw_data.reset_index(inplace=True)\n",
    "\n",
    "    year_key = '2011_2015' if not re.search(r\"2009|2010\", file_name) else '2010' if re.search(r\"2010\", file_name) else '2009'\n",
    "\n",
    "    if year_key == '2011_2015':\n",
    "        convert_id_to_latlon(raw_data)\n",
    "\n",
    "    cleaned_data = raw_data[columns_by_year[year_key]]\n",
    "    unified_column_names = {columns_by_year[year_key][i]: columns_by_year['2011_2015'][i] for i in range(len(columns_by_year[year_key]))}\n",
    "    cleaned_data.rename(columns=unified_column_names, inplace=True)\n",
    "\n",
    "    cleaned_data[columns_by_year['2011_2015'][:8]] = cleaned_data[columns_by_year['2011_2015'][:8]].replace(0.0, None)\n",
    "    cleaned_data.dropna(inplace=True)\n",
    "    \n",
    "    #generate a sampling of Taxi data that's roughly equal to the Uber dataset\n",
    "    #Uber, 200,000/78=2564. \n",
    "    cleaned_data = cleaned_data.sample(2564)\n",
    "    cleaned_data.reset_index(inplace=True)\n",
    "\n",
    "    process_datetime(cleaned_data)\n",
    "    add_distance_column(cleaned_data)\n",
    "    cleaned_data.drop([\"index\"], axis=1, inplace=True)\n",
    "\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c2e9bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    \"\"\"\n",
    "    Get and clean taxi data from 2009-01 to 2015-06\n",
    "    \n",
    "    Returns:\n",
    "    taxi_data -- a dataframe contains all taxi data from 2009-01 to 2015-06\n",
    "    \n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = find_taxi_parquet_urls()\n",
    "    for parquet_url in all_parquet_urls:\n",
    "\n",
    "        dataframe = get_and_clean_month_taxi_data(parquet_url)\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "\n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lat_log_column(dataframe):\n",
    "    \n",
    "    print('add_lat_log_column')\n",
    "    dftaxi = gpd.read_file('taxi_zones.shp')\n",
    "    dftaxi = dftaxi.to_crs(CRS)\n",
    "    \n",
    "    lat1=[]\n",
    "    lon1=[]\n",
    "    lat2=[]\n",
    "    lon2=[]\n",
    "    for LocationID in dataframe[\"PULocationID\"]:\n",
    "        lat=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.x\n",
    "        lon=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.y\n",
    "        if lat.empty:\n",
    "            lat1.append(0)\n",
    "        else:\n",
    "            lat1.append(lat[0])\n",
    "        if lon.empty:\n",
    "            lon1.append(0)\n",
    "        else:\n",
    "            lon1.append(log[0])\n",
    "    \n",
    "    for LocationID in dataframe[\"DOLocationID\"]:\n",
    "        lat=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.x\n",
    "        lon=dftaxi[dftaxi[\"LocationID\"]==LocationID].geometry.centroid.y\n",
    "        if lat.empty:\n",
    "            lat2.append(0)\n",
    "        else:\n",
    "            lat2.append(lat[0])\n",
    "        if log.empty:\n",
    "            lon2.append(0)\n",
    "        else:\n",
    "            lon2.append(lon[0])\n",
    "    dataframe['pickup_latitude']=lat1\n",
    "    dataframe['pickup_longitude']=lon1\n",
    "    dataframe['dropoff_latitude']=lat2\n",
    "    dataframe['dropoff_longitude']=lon2\n",
    "    dataframe.to_csv(\"2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url):\n",
    "    \n",
    "    reponse = requests.get(url)\n",
    "\n",
    "    filename=url.split('/')[-1]\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(reponse.content)\n",
    "    \n",
    "    df = pd.read_parquet(filename)\n",
    "    print(filename)\n",
    "    print(df.columns)\n",
    "    df = df.sample(n=sample_size,ignore_index=True)\n",
    "    try:\n",
    "        if \"PULocationID\" in df.columns:\n",
    "            add_latlog_column(df)\n",
    "        df = df[columns]\n",
    "    except:\n",
    "        try:\n",
    "            df = df[columns2]\n",
    "        except:\n",
    "            try:\n",
    "                df = df[columns3]\n",
    "            except:\n",
    "                add_latlog_column(df)\n",
    "                df = df[columns2]\n",
    "    df.columns = columns2\n",
    "    df = df.sample(n=sample_size,random_state = 1,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "63979eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = get_taxi_parquet_urls()\n",
    "    \n",
    "    for parquet_url in all_parquet_urls:\n",
    "        \n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        \n",
    "        # add a new column to calculate the distance of the taxi ride using latitude and longitude information\n",
    "        add_distance_column(dataframe)\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "    \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "200776ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-434621000313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtaxi_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_and_clean_taxi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-103-c015c73b01b4>\u001b[0m in \u001b[0;36mget_and_clean_taxi_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparquet_url\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_parquet_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_and_clean_month\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# add a new column to calculate the distance of the taxi ride using latitude and longitude information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-09b77735e2ca>\u001b[0m in \u001b[0;36mget_and_clean_month\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     return impl.read(\n\u001b[1;32m    460\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n - \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;34m\"Unable to find a usable engine; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;34m\"tried using: 'pyarrow', 'fastparquet'.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "1. load_and_clean_uber_data\n",
    "\n",
    "this function load data from Uber csv file, and add columns to the pd df \n",
    "and Load data from input file and add date columns to it\n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"\n",
    "    Load data from input file and add date columns to it  \n",
    "    \n",
    "    Arguments:\n",
    "    csv_file -- file name containing the data\n",
    "    \n",
    "    Returns:\n",
    "    pd_data -- a dataframe that contains cleaned data from input file\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load data from Uber CSV file\n",
    "    pd_df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Convert pickup datetime column to pd datetime format\n",
    "    pd_df['pickup_datetime'] = pd.to_datetime(pd_df['pickup_datetime'])\n",
    "    \n",
    "    # get year, month, week, day, hour from pickup datetime column\n",
    "    pd_df['YEAR'] = pd_df['pickup_datetime'].dt.year.astype(int)\n",
    "    pd_df['MONTH'] = pd_df['pickup_datetime'].dt.month.astype(int)\n",
    "    pd_df['WEEK'] = pd_df['pickup_datetime'].dt.dayofweek + 1\n",
    "    pd_df['DAY'] = pd_df['pickup_datetime'].dt.day.astype(int)\n",
    "    pd_df['HOUR'] = pd_df['pickup_datetime'].dt.hour.astype(int)\n",
    "    \n",
    "    pd_df = pd_df.reset_index(drop=True)\n",
    "    \n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    \"\"\"\n",
    "    Load and clean Uber data, and add a column for distance in miles\n",
    "    \n",
    "    Returns:\n",
    "    uber_dataframe -- a dataframe containing cleaned and processed Uber data\n",
    "    \n",
    "    \"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    \n",
    "    valid_columns = [\"fare_amount\", \"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "                     \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "    \n",
    "    # put 0.0 values in valid columns with NaN\n",
    "    uber_dataframe[valid_columns] = uber_dataframe[valid_columns].replace(0.0, np.nan)\n",
    "    \n",
    "    # Drop rows with NaN values in valid columns\n",
    "    uber_dataframe.dropna(subset=valid_columns, inplace=True)\n",
    "    uber_dataframe = uber_dataframe.reset_index(drop=True)\n",
    "    \n",
    "    # Add a new column for the distance traveled during each Uber ride\n",
    "    add_distance_column(uber_dataframe)\n",
    "    \n",
    "    valid_columns.append(\"cal_distance\")\n",
    "    valid_columns.remove(\"fare_amount\")\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    uber_dataframe.dropna(subset=valid_columns, inplace=True)\n",
    "    uber_dataframe = uber_dataframe.drop([\"key\", \"fare_amount\"], axis=1)\n",
    "\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-fdf7c371a2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muber_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_uber_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-119-660e3dd73423>\u001b[0m in \u001b[0;36mget_uber_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Add a new column for the distance traveled during each Uber ride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0madd_distance_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muber_dataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mvalid_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cal_distance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-25199e91c351>\u001b[0m in \u001b[0;36madd_distance_column\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Calculate distance for valid rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mvalid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cal_distance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_distance_with_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_coord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Merge valid_df back into the original dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-a340afbbb24a>\u001b[0m in \u001b[0;36mcalculate_distance_with_coords\u001b[0;34m(from_coord, to_coord)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrad_lon1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrad_lon2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     distance_radius = 2 * math.asin(\n\u001b[0;32m----> 9\u001b[0;31m         math.sqrt(math.pow(math.sin(a / 2), 2)+ math.cos(rad_lat1) * math.cos(rad_lat2) * math.pow(math.sin(b / 2), 2)))\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_radius\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mEARTH_RADIUS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot convert the series to {converter}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"__{converter.__name__}__\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'float'>"
     ]
    }
   ],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
