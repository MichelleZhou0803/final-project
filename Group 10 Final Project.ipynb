{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a606159",
   "metadata": {},
   "source": [
    "## Group 10 \n",
    "### Yixuan (Sharon) Qian - yq2348\n",
    "### Michelle Jingyi Zhou - jz3508"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.4.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.4.2) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.4.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.4.2) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.4.2) (1.16.0)\n",
      "Requirement already satisfied: pyarrow==7.0.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (7.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pyarrow==7.0.0) (1.21.5)\n",
      "Requirement already satisfied: geopandas in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (1.4.2)\n",
      "Requirement already satisfied: shapely>=1.7 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (2.0.1)\n",
      "Requirement already satisfied: packaging in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (21.3)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (3.5.0)\n",
      "Requirement already satisfied: fiona>=1.8 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (1.9.3)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: certifi in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2021.10.8)\n",
      "Requirement already satisfied: munch>=2.3.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (4.11.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (21.4.0)\n",
      "Requirement already satisfied: click~=8.0 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (8.0.4)\n",
      "Requirement already satisfied: six in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (1.21.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from packaging->geopandas) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# all import statements needed for the project\n",
    "\n",
    "import math\n",
    "\n",
    "#from math import tan, pi\n",
    "import os\n",
    "\n",
    "import bs4 \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install pandas==1.4.2\n",
    "!pip install pyarrow==7.0.0\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import numpy as np\n",
    "import re\n",
    "import os.path\n",
    "import glob\n",
    "!pip install geopandas\n",
    "import geopandas\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants we might need\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV_FILES = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\",\n",
    "                    \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "\n",
    "EARTH_RADIUS = 6378.137\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "Overview: For Part 1, we downloaded the Parquet files, cleaned and filtered for the relevant data, filling in missing data, and generating samples of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance\n",
    "\n",
    "1.rad(d) function converts numeric degrees to radians\n",
    "\n",
    "2.distance calculation function\n",
    "calculate_distance_with_coords(from_coord, to_coord) calculates the distance btween coordinates\n",
    "\n",
    "3.add_distance_column(dataframe)\n",
    "\n",
    "Add a column call_distance to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a34ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts numeric degrees to radians\n",
    "# d is diameter\n",
    "def rad(d):\n",
    "    return d * math.pi / 180.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbbe6cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T17:32:09.000495Z",
     "start_time": "2023-04-24T17:32:08.965159Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    \"\"\"\n",
    "     this function calculates the distance between two points\n",
    "     using their latitude and longitude coordinates\n",
    "     \n",
    "     Output: distance ----- distnace between two coordinates\n",
    "    \"\"\"\n",
    "    \n",
    "    rad_lat1 = rad(from_coord[0])\n",
    "    rad_lon1 = rad(from_coord[1])\n",
    "    rad_lat2 = rad(to_coord[0])\n",
    "    rad_lon2 = rad(to_coord[1])\n",
    "    \n",
    "    a = rad_lat1 - rad_lat2\n",
    "    b = rad_lon1 - rad_lon2\n",
    "    distance_radius = 2 * math.asin(\n",
    "        math.sqrt(math.pow(math.sin(a / 2), 2) \n",
    "                  + math.cos(rad_lat1) * math.cos(rad_lat2) * math.pow(math.sin(b / 2), 2)))\n",
    "    distance = distance_radius * EARTH_RADIUS\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df):\n",
    "    \"\"\"\n",
    "    Add a column â€˜cal_distance' to the dataframe 'sample_df'\n",
    "\n",
    "    \"\"\"\n",
    "    distance_list = []\n",
    "    for i in range(len(df)):\n",
    "        s_lat, s_lon  = df[\"pickup_latitude\"][i], df[\"pickup_longitude\"][i]\n",
    "        e_lat, e_lon  = df[\"dropoff_latitude\"][i], df[\"dropoff_longitude\"][i]\n",
    "        if s_lat and s_lon and e_lat and e_lon:\n",
    "            outside = True\n",
    "            if s_lat > NEW_YORK_BOX_COORDS[0][0] and s_lat < NEW_YORK_BOX_COORDS[1][0]:\n",
    "                if e_lat > NEW_YORK_BOX_COORDS[0][0] and e_lat < NEW_YORK_BOX_COORDS[1][0]:\n",
    "                    if s_lon > NEW_YORK_BOX_COORDS[0][1] and s_lon < NEW_YORK_BOX_COORDS[1][1]:\n",
    "                        if e_lon > NEW_YORK_BOX_COORDS[0][1] and e_lon < NEW_YORK_BOX_COORDS[1][1]:\n",
    "                            outside = False\n",
    "            if outside:\n",
    "                s_lat, s_lon, e_lat, e_lon = None, None, None, None\n",
    "        from_coord = (s_lat, s_lon)\n",
    "        to_coord = (e_lat, e_lon)\n",
    "        if s_lat and s_lon and e_lat and e_lon:\n",
    "            distance_list.append(calculate_distance_with_coords(from_coord, to_coord))\n",
    "        else:\n",
    "            distance_list.append(None)\n",
    "    df[\"cal_distance\"] = distance_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd328f1e",
   "metadata": {},
   "source": [
    "### Use Taxi Zones Shapefile to Convert to Coordinates\n",
    "\n",
    "1. get_latlon_from_locationID():\n",
    "\n",
    "We load taxi zones shapefile\n",
    "\n",
    "2. convert_id_to_latlon(sample_tables)\n",
    "\n",
    "we convert area ID column into  coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b1721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load taxi zones from a shapefile and add new columns.\n",
    "\n",
    "def get_latlon_from_locationID():\n",
    "    # Read the taxi zone shapefile and convert it to CRS\n",
    "    #gdf is GeoDataFrame\n",
    "    gdf = geopandas.read_file(\"taxi_zones.shp\")\n",
    "    gdf = gdf.to_crs(CRS)\n",
    "    \n",
    "    # Get the lon and lat of the centroid of each  zone\n",
    "    lon = gdf.centroid.x\n",
    "    lat = gdf.centroid.y\n",
    "    \n",
    "    # Add new columns to the dataframe to store the lon and lat\n",
    "    gdf[\"lon\"] = lon\n",
    "    gdf[\"lat\"] = lat\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16056eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id_to_latlon(samples_df):\n",
    "    \"\"\"\n",
    "    Convert area ID column into two coordinates\n",
    "    \"\"\"\n",
    "    gdf = get_latlon_from_locationID()\n",
    "\n",
    "    def get_coords(location_id):\n",
    "        if location_id < 264 and location_id in gdf[\"LocationID\"].values:\n",
    "            row_index = gdf[gdf[\"LocationID\"] == location_id].index.values[0]\n",
    "            lon, lat = float(gdf[\"lon\"][row_index]), float(gdf[\"lat\"][row_index])\n",
    "\n",
    "            if (\n",
    "                NEW_YORK_BOX_COORDS[0][0] < lat < NEW_YORK_BOX_COORDS[1][0]\n",
    "                and NEW_YORK_BOX_COORDS[0][1] < lon < NEW_YORK_BOX_COORDS[1][1]\n",
    "            ):\n",
    "                return lon, lat\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    start_coords = samples_df[\"PULocationID\"].apply(get_coords)\n",
    "    end_coords = samples_df[\"DOLocationID\"].apply(get_coords)\n",
    "\n",
    "    samples_df[\"pickup_longitude\"] = [coord[0] for coord in start_coords]\n",
    "    samples_df[\"pickup_latitude\"] = [coord[1] for coord in start_coords]\n",
    "    samples_df[\"dropoff_longitude\"] = [coord[0] for coord in end_coords]\n",
    "    samples_df[\"dropoff_latitude\"] = [coord[1] for coord in end_coords]\n",
    "    return samples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data\n",
    "\n",
    "1. this function programmatically downloads the Yellow Taxi Parquet files for a specific date range 2009-01 and 2015-06 from the website. it returns a list that contains all taxi data url in TAXI_URL \"\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "3. we added latitude and logitude from taxi_zones. We also added pickup_latitude, dropoff_latitude, dropoff_latitude and dropoff_longitude as columns to the dataframe for convenient calculation\n",
    "\n",
    "4. Download Parquet files, get some sample from these files, Clean the dataframe according to existing location IDs, Write data into .csv and return dataframe\n",
    "\n",
    "5. get_and_clean_taxi_data\n",
    "Get taxi data. If taxi.csv exists, read-only. Otherwise, download data and generate taxi.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_parquet_urls():\n",
    "\n",
    "    parquet_urls = []\n",
    "    response = requests.get(url=TAXI_URL)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            url = link['href']\n",
    "            if 'yellow_tripdata' in url and '.parquet' in url:\n",
    "                date_str = url.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "                year = int(date_str[:4])\n",
    "                month = int(date_str[4:6])\n",
    "                if year < 2015 or (year == 2015 and month <= 6):\n",
    "                    parquet_urls.append(url)\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5509ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datetime(samples_df):\n",
    "    \"\"\"\n",
    "    Change column type for date column and add columns for specific time data\n",
    "\n",
    "    Arguments:\n",
    "    df -- a dataframe with time data column\n",
    "\n",
    "    \"\"\"\n",
    "    if \"tpep_pickup_datetime\" in samples_df.columns:\n",
    "        samples_df['tpep_pickup_datetime'] = pd.to_datetime(samples_df['tpep_pickup_datetime'])\n",
    "        samples_df['tpep_dropoff_datetime'] = pd.to_datetime(samples_df['tpep_dropoff_datetime'])\n",
    "\n",
    "        datetime_columns = {\n",
    "            'DATE': samples_df['tpep_pickup_datetime'],\n",
    "            'YEAR': samples_df['tpep_pickup_datetime'].dt.year.astype(int),\n",
    "            'MONTH': samples_df['tpep_pickup_datetime'].dt.month.astype(int),\n",
    "            'DAY': samples_df['tpep_pickup_datetime'].dt.day.astype(int),\n",
    "            'HOUR': samples_df['tpep_pickup_datetime'].dt.hour.astype(int),\n",
    "            'WEEK': samples_df['tpep_pickup_datetime'].dt.dayofweek + 1 \n",
    "        }\n",
    "    else:\n",
    "        datetime_columns = {\n",
    "            'tpep_pickup_datetime': None,\n",
    "            'YEAR': None,\n",
    "            'MONTH': None,\n",
    "            'DAY': None,\n",
    "            'HOUR': None,\n",
    "            'WEEK': None\n",
    "        } \n",
    "    samples_df = samples_df.assign(**datetime_columns) \n",
    "    return samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d9aab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    \"\"\"\n",
    "    Download, read, sample, and clean one month of taxi data\n",
    "\n",
    "    Arguments:\n",
    "    url -- a url for downloading a specific month's taxi parquet file\n",
    "\n",
    "    Returns:\n",
    "    cleaned_data -- a DataFrame containing cleaned taxi data for one month\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "    columns_by_year = {\n",
    "        '2011_2015': [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"pickup_longitude\",\n",
    "                  \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\"],\n",
    "        '2010': [\"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "               \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\"],\n",
    "        '2009': [\"Trip_Pickup_DateTime\", \"Trip_Dropoff_DateTime\", \"Passenger_Count\", \"Trip_Distance\", \"Start_Lon\", \"Start_Lat\",\n",
    "               \"End_Lon\", \"End_Lat\", \"Tip_Amt\"]\n",
    "    }\n",
    "    \n",
    "    print('read data from ', file_name)\n",
    "    raw_data = pd.read_parquet(file_name)\n",
    "    # to speed up the data clean process\n",
    "    raw_data = raw_data.sample(4000)\n",
    "    raw_data.reset_index(inplace=True)\n",
    "\n",
    "    year_key = '2011_2015' if not re.search(r\"2009|2010\", file_name) else '2010' if re.search(r\"2010\", file_name) else '2009'\n",
    "\n",
    "    if year_key == '2011_2015':\n",
    "        raw_data = convert_id_to_latlon(raw_data)\n",
    "\n",
    "    cleaned_data = raw_data[columns_by_year[year_key]]\n",
    "    unified_column_names = {columns_by_year[year_key][i]: columns_by_year['2011_2015'][i] for i in range(len(columns_by_year[year_key]))}\n",
    "    cleaned_data.rename(columns=unified_column_names, inplace=True)\n",
    "\n",
    "    cleaned_data[columns_by_year['2011_2015'][:8]] = cleaned_data[columns_by_year['2011_2015'][:8]].replace(0.0, None)\n",
    "    cleaned_data.dropna(inplace=True)\n",
    "    \n",
    "    #generate a sampling of Taxi data that's roughly equal to the Uber dataset\n",
    "    #Uber, 200,000/78=2564. \n",
    "    cleaned_data = cleaned_data.sample(2564)\n",
    "    cleaned_data.reset_index(inplace=True)\n",
    "\n",
    "    cleaned_data = process_datetime(cleaned_data)\n",
    "    cleaned_data = add_distance_column(cleaned_data)\n",
    "    cleaned_data.drop([\"index\"], axis=1, inplace=True)\n",
    "\n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63979eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = get_taxi_parquet_urls()\n",
    "    # print(all_parquet_urls)\n",
    "    for parquet_url in all_parquet_urls:\n",
    "        \n",
    "        dataframe = get_and_clean_month_taxi_data(parquet_url)\n",
    "                \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "    \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "200776ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data from  yellow_tripdata_2015-01.parquet\n",
      "read data from  yellow_tripdata_2015-02.parquet\n",
      "read data from  yellow_tripdata_2015-03.parquet\n",
      "read data from  yellow_tripdata_2015-04.parquet\n",
      "read data from  yellow_tripdata_2015-05.parquet\n",
      "read data from  yellow_tripdata_2015-06.parquet\n",
      "read data from  yellow_tripdata_2015-07.parquet\n",
      "read data from  yellow_tripdata_2015-08.parquet\n",
      "read data from  yellow_tripdata_2015-09.parquet\n",
      "read data from  yellow_tripdata_2015-10.parquet\n",
      "read data from  yellow_tripdata_2015-11.parquet\n",
      "read data from  yellow_tripdata_2015-12.parquet\n",
      "read data from  yellow_tripdata_2014-01.parquet\n",
      "read data from  yellow_tripdata_2014-02.parquet\n",
      "read data from  yellow_tripdata_2014-03.parquet\n",
      "read data from  yellow_tripdata_2014-04.parquet\n",
      "read data from  yellow_tripdata_2014-05.parquet\n",
      "read data from  yellow_tripdata_2014-06.parquet\n",
      "read data from  yellow_tripdata_2014-07.parquet\n",
      "read data from  yellow_tripdata_2014-08.parquet\n",
      "read data from  yellow_tripdata_2014-09.parquet\n",
      "read data from  yellow_tripdata_2014-10.parquet\n",
      "read data from  yellow_tripdata_2014-11.parquet\n",
      "read data from  yellow_tripdata_2014-12.parquet\n",
      "read data from  yellow_tripdata_2013-01.parquet\n",
      "read data from  yellow_tripdata_2013-02.parquet\n",
      "read data from  yellow_tripdata_2013-03.parquet\n",
      "read data from  yellow_tripdata_2013-04.parquet\n",
      "read data from  yellow_tripdata_2013-05.parquet\n",
      "read data from  yellow_tripdata_2013-06.parquet\n",
      "read data from  yellow_tripdata_2013-07.parquet\n",
      "read data from  yellow_tripdata_2013-08.parquet\n",
      "read data from  yellow_tripdata_2013-09.parquet\n",
      "read data from  yellow_tripdata_2013-10.parquet\n",
      "read data from  yellow_tripdata_2013-11.parquet\n",
      "read data from  yellow_tripdata_2013-12.parquet\n",
      "read data from  yellow_tripdata_2012-01.parquet\n",
      "read data from  yellow_tripdata_2012-02.parquet\n",
      "read data from  yellow_tripdata_2012-03.parquet\n",
      "read data from  yellow_tripdata_2012-04.parquet\n",
      "read data from  yellow_tripdata_2012-05.parquet\n",
      "read data from  yellow_tripdata_2012-06.parquet\n",
      "read data from  yellow_tripdata_2012-07.parquet\n",
      "read data from  yellow_tripdata_2012-08.parquet\n",
      "read data from  yellow_tripdata_2012-09.parquet\n",
      "read data from  yellow_tripdata_2012-10.parquet\n",
      "read data from  yellow_tripdata_2012-11.parquet\n",
      "read data from  yellow_tripdata_2012-12.parquet\n",
      "read data from  yellow_tripdata_2011-01.parquet\n",
      "read data from  yellow_tripdata_2011-02.parquet\n",
      "read data from  yellow_tripdata_2011-03.parquet\n",
      "read data from  yellow_tripdata_2011-04.parquet\n",
      "read data from  yellow_tripdata_2011-05.parquet\n",
      "read data from  yellow_tripdata_2011-06.parquet\n",
      "read data from  yellow_tripdata_2011-07.parquet\n",
      "read data from  yellow_tripdata_2011-08.parquet\n",
      "read data from  yellow_tripdata_2011-09.parquet\n",
      "read data from  yellow_tripdata_2011-10.parquet\n",
      "read data from  yellow_tripdata_2011-11.parquet\n",
      "read data from  yellow_tripdata_2011-12.parquet\n",
      "read data from  yellow_tripdata_2010-01.parquet\n",
      "read data from  yellow_tripdata_2010-02.parquet\n",
      "read data from  yellow_tripdata_2010-03.parquet\n",
      "read data from  yellow_tripdata_2010-04.parquet\n",
      "read data from  yellow_tripdata_2010-05.parquet\n",
      "read data from  yellow_tripdata_2010-06.parquet\n",
      "read data from  yellow_tripdata_2010-07.parquet\n",
      "read data from  yellow_tripdata_2010-08.parquet\n",
      "read data from  yellow_tripdata_2010-09.parquet\n",
      "read data from  yellow_tripdata_2010-10.parquet\n",
      "read data from  yellow_tripdata_2010-11.parquet\n",
      "read data from  yellow_tripdata_2010-12.parquet\n",
      "read data from  yellow_tripdata_2009-01.parquet\n",
      "read data from  yellow_tripdata_2009-02.parquet\n",
      "read data from  yellow_tripdata_2009-03.parquet\n",
      "read data from  yellow_tripdata_2009-04.parquet\n",
      "read data from  yellow_tripdata_2009-05.parquet\n",
      "read data from  yellow_tripdata_2009-06.parquet\n",
      "read data from  yellow_tripdata_2009-07.parquet\n",
      "read data from  yellow_tripdata_2009-08.parquet\n",
      "read data from  yellow_tripdata_2009-09.parquet\n",
      "read data from  yellow_tripdata_2009-10.parquet\n",
      "read data from  yellow_tripdata_2009-11.parquet\n",
      "read data from  yellow_tripdata_2009-12.parquet\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('/content/taxi_clean_data.csv'): \n",
    "    taxi_data = pd.read_csv('/content/taxi_clean_data.csv')\n",
    "    taxi_data = taxi_data.loc[:, ~taxi_data.columns.str.contains('^Unnamed')]\n",
    "else:\n",
    "    taxi_data = get_and_clean_taxi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "1. load_and_clean_uber_data\n",
    "\n",
    "this function load data from Uber csv file, and add columns to the pd df \n",
    "and Load data from input file and add date columns to it\n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"\n",
    "    Load data from input file and add date columns to it  \n",
    "    \n",
    "    Arguments:\n",
    "    csv_file -- file name containing the data\n",
    "    \n",
    "    Returns:\n",
    "    pd_data -- a dataframe that contains cleaned data from input file\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load data from Uber CSV file\n",
    "    pd_df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Convert pickup datetime column to pd datetime format\n",
    "    pd_df['pickup_datetime'] = pd.to_datetime(pd_df['pickup_datetime'])\n",
    "    \n",
    "    # get year, month, week, day, hour from pickup datetime column\n",
    "    pd_df['YEAR'] = pd_df['pickup_datetime'].dt.year.astype(int)\n",
    "    pd_df['MONTH'] = pd_df['pickup_datetime'].dt.month.astype(int)\n",
    "    pd_df['WEEK'] = pd_df['pickup_datetime'].dt.dayofweek + 1\n",
    "    pd_df['DAY'] = pd_df['pickup_datetime'].dt.day.astype(int)\n",
    "    pd_df['HOUR'] = pd_df['pickup_datetime'].dt.hour.astype(int)\n",
    "    \n",
    "    pd_df = pd_df.reset_index(drop=True)\n",
    "    \n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    \"\"\"\n",
    "    Load and clean Uber data, and add a column for distance in miles\n",
    "    \n",
    "    Returns:\n",
    "    uber_dataframe -- a dataframe containing cleaned and processed Uber data\n",
    "    \n",
    "    \"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    \n",
    "    valid_columns = [\"fare_amount\", \"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "                     \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "    \n",
    "    # put 0.0 values in valid columns with NaN\n",
    "    uber_dataframe[valid_columns] = uber_dataframe[valid_columns].replace(0.0, None)\n",
    "    \n",
    "    # Drop rows with NaN values in valid columns\n",
    "    uber_dataframe.dropna(subset=valid_columns, inplace=True)\n",
    "    uber_dataframe = uber_dataframe.reset_index(drop=True)\n",
    "    \n",
    "    # Add a new column for the distance traveled during each Uber ride\n",
    "    add_distance_column(uber_dataframe)\n",
    "    \n",
    "    valid_columns.append(\"cal_distance\")\n",
    "    valid_columns.remove(\"fare_amount\")\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    uber_dataframe.dropna(subset=valid_columns, inplace=True)\n",
    "    uber_dataframe = uber_dataframe.drop([\"key\", \"fare_amount\"], axis=1)\n",
    "\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e566081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (7.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pyarrow) (1.21.5)\n",
      "Requirement already satisfied: fastparquet in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (2023.2.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (1.21.5)\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (2.6.2)\n",
      "Collecting pandas>=1.5.0\n",
      "  Using cached pandas-2.0.1-cp39-cp39-macosx_10_9_x86_64.whl (11.8 MB)\n",
      "Requirement already satisfied: packaging in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (21.3)\n",
      "Requirement already satisfied: fsspec in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from fastparquet) (2022.2.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/michellezhou/opt/anaconda3/lib/python3.9/site-packages (from packaging->fastparquet) (3.0.4)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.2\n",
      "    Uninstalling pandas-1.4.2:\n",
      "      Successfully uninstalled pandas-1.4.2\n",
      "Successfully installed pandas-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "339997e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>cal_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24238194</td>\n",
       "      <td>2015-05-07 19:52:06+00:00</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>1.685209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27835199</td>\n",
       "      <td>2009-07-17 20:04:56+00:00</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.99471</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>2.460343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44984355</td>\n",
       "      <td>2009-08-24 21:45:00+00:00</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.74077</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>5.042019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25894730</td>\n",
       "      <td>2009-06-26 08:22:21+00:00</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>1.663545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17610152</td>\n",
       "      <td>2014-08-28 17:47:00+00:00</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>4.480464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           pickup_datetime pickup_longitude pickup_latitude  \\\n",
       "0    24238194 2015-05-07 19:52:06+00:00       -73.999817       40.738354   \n",
       "1    27835199 2009-07-17 20:04:56+00:00       -73.994355       40.728225   \n",
       "2    44984355 2009-08-24 21:45:00+00:00       -74.005043        40.74077   \n",
       "3    25894730 2009-06-26 08:22:21+00:00       -73.976124       40.790844   \n",
       "4    17610152 2014-08-28 17:47:00+00:00       -73.925023       40.744085   \n",
       "\n",
       "  dropoff_longitude dropoff_latitude  passenger_count  YEAR  MONTH  WEEK  DAY  \\\n",
       "0        -73.999512        40.723217                1  2015      5     4    7   \n",
       "1         -73.99471        40.750325                1  2009      7     5   17   \n",
       "2        -73.962565        40.772647                1  2009      8     1   24   \n",
       "3        -73.965316        40.803349                3  2009      6     5   26   \n",
       "4        -73.973082        40.761247                5  2014      8     4   28   \n",
       "\n",
       "   HOUR  cal_distance  \n",
       "0    19      1.685209  \n",
       "1    20      2.460343  \n",
       "2    21      5.042019  \n",
       "3     8      1.663545  \n",
       "4    17      4.480464  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data = get_uber_data()\n",
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "1. function get_all_weather_csvs(directory)\n",
    "\n",
    "\n",
    "\n",
    "2. function clean_month_weather_data_hourly(csv_file):\n",
    "\n",
    "\n",
    "3. function clean_month_weather_data_daily(csv_file):\n",
    "\n",
    "\n",
    "4. function load_and_clean_weather_data():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    csv_files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\") and \"weather\" in file.lower():\n",
    "            csv_files.append(os.path.join(directory, file))\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "\n",
    "    columns_to_keep = [\"DATE\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]\n",
    "    weather_data = pd.read_csv(csv_file, usecols=columns_to_keep)\n",
    "\n",
    "    # Replace missing values in HourlyPrecipitation with 0.0\n",
    "    weather_data[\"HourlyPrecipitation\"] = weather_data[\"HourlyPrecipitation\"].fillna(0)\n",
    "\n",
    "    # Convert DATE column to datetime and extract year, month, day, and hour\n",
    "    weather_data[\"DATE\"] = pd.to_datetime(weather_data[\"DATE\"])\n",
    "    weather_data[\"YEAR\"] = weather_data[\"DATE\"].dt.year.astype(int)\n",
    "    weather_data[\"MONTH\"] = weather_data[\"DATE\"].dt.month.astype(int)\n",
    "    weather_data[\"WEEK\"] = weather_data[\"DATE\"].dt.dayofweek + 1\n",
    "    weather_data[\"DAY\"] = weather_data[\"DATE\"].dt.day.astype(int)\n",
    "    weather_data[\"HOUR\"] = weather_data[\"DATE\"].dt.hour.astype(int)\n",
    "\n",
    "\n",
    "    # gethourly weather data for each day\n",
    "    hourly_weather = []\n",
    "    hourly_weather_columns = list(weather_data.columns)\n",
    "    date_string = \"\"\n",
    "    for i in range(weather_data.shape[0]):\n",
    "        tmp_date_string = str(weather_data.iloc[i, :][\"YEAR\"]) + str(weather_data.iloc[i, :][\"MONTH\"]) + str(weather_data.iloc[i, :][\"DAY\"]) + str(weather_data.iloc[i, :][\"HOUR\"])\n",
    "        if tmp_date_string == date_string:\n",
    "            continue\n",
    "        else:\n",
    "            hourly_weather.append(weather_data.iloc[i, :].to_list())\n",
    "            date_string = tmp_date_string\n",
    "\n",
    "    # Create a new df from the hourly weather data and ignore any rows with no values\n",
    "    hourly_weather_data = pd.DataFrame(hourly_weather, columns=hourly_weather_columns)\n",
    "    hourly_weather_data.dropna(inplace=True)\n",
    "\n",
    "    return hourly_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \n",
    "    columns_to_keep = [\"DATE\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\", \"REPORT_TYPE\"]\n",
    "    \n",
    "    weather_data = pd.read_csv(csv_file, usecols=columns_to_keep)\n",
    "    weather_data.dropna(subset=[\"DailyAverageWindSpeed\"], inplace=True)\n",
    "    weather_data[[\"DailyPrecipitation\"]] = weather_data[[\"DailyPrecipitation\"]].fillna(0)\n",
    "    \n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "    weather_data['YEAR'] = weather_data['DATE'].dt.year.astype(int)\n",
    "    weather_data['MONTH'] = weather_data['DATE'].dt.month.astype(int)\n",
    "    weather_data[\"WEEK\"] = weather_data['DATE'].dt.dayofweek + 1\n",
    "    weather_data['DAY'] = weather_data['DATE'].dt.day.astype(int)\n",
    "\n",
    "    \n",
    "    date_str = \"\"\n",
    "    daily_weather = []\n",
    "    daily_weather_columns = list(weather_data.columns)\n",
    "    for i in range(weather_data.shape[0]):\n",
    "        tmp_date_str = str(weather_data.iloc[i,:][\"YEAR\"]) + str(weather_data.iloc[i,:][\"MONTH\"]) + str(weather_data.iloc[i,:][\"DAY\"])\n",
    "        if tmp_date_str == date_str:\n",
    "            continue\n",
    "        else:\n",
    "            daily_weather.append(weather_data.iloc[i,:].to_list())\n",
    "            date_str = tmp_date_str \n",
    "    \n",
    "    daily_weather_data = pd.DataFrame(daily_weather, columns=daily_weather_columns)\n",
    "    \n",
    "    return daily_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in WEATHER_CSV_FILES:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 03:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 04:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 DATE HourlyPrecipitation  HourlyWindSpeed  YEAR  MONTH  WEEK  \\\n",
       "0 2009-01-01 00:51:00                   0             18.0  2009      1     4   \n",
       "1 2009-01-01 01:51:00                   0             18.0  2009      1     4   \n",
       "2 2009-01-01 02:51:00                   0             18.0  2009      1     4   \n",
       "3 2009-01-01 03:51:00                   0              8.0  2009      1     4   \n",
       "4 2009-01-01 04:51:00                   0             11.0  2009      1     4   \n",
       "\n",
       "   DAY  HOUR  \n",
       "0    1     0  \n",
       "1    1     1  \n",
       "2    1     2  \n",
       "3    1     3  \n",
       "4    1     4  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>REPORT_TYPE</th>\n",
       "      <th>DailyAverageWindSpeed</th>\n",
       "      <th>DailyPrecipitation</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-31 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-08-01 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-08-02 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-08-03 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-08-04 23:59:00</td>\n",
       "      <td>SOD</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2012</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DATE REPORT_TYPE DailyAverageWindSpeed DailyPrecipitation  \\\n",
       "0  2012-07-31 23:59:00       SOD                     3.8               0.00   \n",
       "1  2012-08-01 23:59:00       SOD                     2.3               0.64   \n",
       "2  2012-08-02 23:59:00       SOD                     2.7               0.00   \n",
       "3  2012-08-03 23:59:00       SOD                     3.5               0.00   \n",
       "4  2012-08-04 23:59:00       SOD                     3.1               0.00   \n",
       "\n",
       "   YEAR MONTH WEEK DAY  \n",
       "0  2012     7    2  31  \n",
       "1  2012     8    3   1  \n",
       "2  2012     8    4   2  \n",
       "3  2012     8    5   3  \n",
       "4  2012     8    6   4  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "create table if not exists hourly_weather(\n",
    "  hid int primary key, \n",
    "  date timestamp, \n",
    "  year int, \n",
    "  month int, \n",
    "  week int, \n",
    "  hour int, \n",
    "  hourlyPrecipitation float, \n",
    "  hourlyWindSpeed float\n",
    ")\n",
    "\"\"\"\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "create table if not exists daily_weather(\n",
    "  did int primary key,\n",
    "  date timestamp,\n",
    "  dailyPrecipitation float,\n",
    "  dailyAverageWindSpeed float,\n",
    "  year int,\n",
    "  month int,\n",
    "  day int, \n",
    "  week int \n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "create table if not exists taxi_trip(\n",
    "  tid int primary key,\n",
    "  date timestamp,\n",
    "  tpep_pickup_datetime timestamp,\n",
    "  tpep_dropoff_datetime timestamp,\n",
    "  trip_distance float,\n",
    "  tip_amount float,\n",
    "  passenger_count int,\n",
    "  pickup_longitude float,\n",
    "  pickup_latitude float,\n",
    "  dropoff_longitude float,\n",
    "  dropoff_latitude float,\n",
    "  cal_distance float,\n",
    "  year int,\n",
    "  month int,\n",
    "  day int,\n",
    "  hour int,\n",
    "  week int\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "create table if not exists uber_trip(\n",
    "  uid int primary key,\n",
    "  pickup_datetime timestamp,\n",
    "  pickup_longitude float,\n",
    "  pickup_latitude float,\n",
    "  dropoff_longitude float,\n",
    "  dropoff_latitude float,\n",
    "  passenger_count int,\n",
    "  year int,\n",
    "  month int,\n",
    "  day int,\n",
    "  week int,\n",
    "  hour int,\n",
    "  cal_distance float\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x7fcefe2fc3a0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the tables with the schema files\n",
    "from sqlalchemy import text\n",
    "conn = engine.connect()\n",
    "conn.execute(text('drop table if exists taxi_trip'))\n",
    "conn.execute(text('drop table if exists uber_trip'))\n",
    "conn.execute(text('drop table if exists hourly_weather'))\n",
    "conn.execute(text('drop table if exists daily_weather'))\n",
    "conn.execute(text(HOURLY_WEATHER_SCHEMA))\n",
    "conn.execute(text(DAILY_WEATHER_SCHEMA))\n",
    "conn.execute(text(TAXI_TRIPS_SCHEMA))\n",
    "conn.execute(text(UBER_TRIPS_SCHEMA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for table in table_to_df_dict.keys():\n",
    "        table_to_df_dict[table].to_sql(table, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(f'{QUERY_DIRECTORY}/{outfile}', 'w') as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "For 01-2009 through 06-2015, show the popularity of Yellow Taxi rides for each hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT HOUR, COUNT(*) as count \n",
    "FROM taxi_trips\n",
    "WHERE tpep_pickup_datetime between '2009-01-01' and '2015-06-30'\n",
    "GROUP BY HOUR \n",
    "ORDER BY count DESC \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19, 12539),\n",
       " (18, 12224),\n",
       " (20, 11916),\n",
       " (21, 11507),\n",
       " (22, 11120),\n",
       " (14, 9977),\n",
       " (23, 9921),\n",
       " (12, 9842),\n",
       " (17, 9783),\n",
       " (13, 9763),\n",
       " (15, 9746),\n",
       " (9, 9304),\n",
       " (11, 9202),\n",
       " (10, 9028),\n",
       " (8, 9005),\n",
       " (16, 8103),\n",
       " (0, 8071),\n",
       " (7, 7217),\n",
       " (1, 5760),\n",
       " (2, 4291),\n",
       " (6, 4152),\n",
       " (3, 3145),\n",
       " (4, 2390),\n",
       " (5, 1905)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950daa00",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "For the same time frame, show the popularity of Uber rides for each day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "31b1cb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 30166),\n",
       " (6, 29598),\n",
       " (4, 29338),\n",
       " (3, 28328),\n",
       " (2, 27461),\n",
       " (7, 25834),\n",
       " (1, 24681)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_2_FILENAME = \"query2.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT WEEK, COUNT(*) as count \n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime >= '2009-01-01' and pickup_datetime <= '2015-06-30'\n",
    "GROUP BY WEEK \n",
    "ORDER BY count DESC \n",
    "\"\"\"\n",
    "query2_result = conn.execute(text(QUERY_2)).fetchall()\n",
    "query2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1a2112a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4611729",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "What is the 95% percentile of distance traveled for all hired trips during July 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f354c1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10.24292993993939,)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_3_FILENAME = \"query3.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "WITH t as (\n",
    "  SELECT cal_distance FROM taxi_trips where tpep_pickup_datetime >= '2013-07-01' and tpep_pickup_datetime <= '2013-07-31'\n",
    "  UNION ALL\n",
    "  SELECT cal_distance FROM uber_trips where pickup_datetime >= '2013-07-01' and pickup_datetime <= '2013-07-31' \n",
    ") \n",
    "SELECT cal_distance from t\n",
    "ORDER BY cal_distance asc\n",
    "LIMIT 1\n",
    "OFFSET (SELECT COUNT(*) FROM t) * 95/100 - 1;\n",
    "\"\"\"\n",
    "query3_result = conn.execute(text(QUERY_3)).fetchall()\n",
    "query3_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7c15b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603af02",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "What were the top 10 days with the highest number of hired rides for 2009, and what was the\n",
    "average distance for each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1474487e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2009-12-11', 230, 2.550094204732856),\n",
       " ('2009-02-20', 222, 2.313034900065012),\n",
       " ('2009-10-23', 220, 2.5649608141938214),\n",
       " ('2009-07-23', 213, 3.291623721759143),\n",
       " ('2009-01-31', 212, 2.560513214631675),\n",
       " ('2009-08-14', 210, 3.5150407367149175),\n",
       " ('2009-04-30', 208, 3.2345551001963915),\n",
       " ('2009-06-05', 206, 2.5656986990750736),\n",
       " ('2009-04-03', 204, 2.795264035210946),\n",
       " ('2009-12-15', 202, 3.2724666089826204)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_4_FILENAME = \"query4.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "WITH t as (\n",
    "  SELECT date(tpep_pickup_datetime) as date, trip_distance from taxi_trips\n",
    "  UNION ALL\n",
    "  SELECT date(pickup_datetime) as date, cal_distance as trip_distance from uber_trips \n",
    ") \n",
    "SELECT date, COUNT(*), avg(trip_distance) FROM t\n",
    "WHERE date >= '2009-01-01' and date <= '2009-12-31'\n",
    "GROUP by date\n",
    "ORDER BY count(*) desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "query4_result = conn.execute(text(QUERY_4)).fetchall()\n",
    "query4_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c67c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a3a8d7",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "Which 10 days in 2014 were the windiest on average, and how many hired trips were made on\n",
    "those days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c4fea0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014-03-13', 202),\n",
       " ('2014-01-07', 143),\n",
       " ('2014-02-13', 133),\n",
       " ('2014-01-02', 124),\n",
       " ('2014-03-26', 181),\n",
       " ('2014-12-07', 174),\n",
       " ('2014-12-08', 159),\n",
       " ('2014-03-29', 200),\n",
       " ('2014-11-02', 162),\n",
       " ('2014-01-03', 84)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_5_FILENAME = \"query5.sql\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "WITH t1 as (\n",
    "  SELECT date(date) as date FROM daily_weather \n",
    "  WHERE date >= '2014-01-01' and date <= '2014-12-31'\n",
    "  ORDER BY dailyAverageWindSpeed desc\n",
    "  LIMIT 10  \n",
    "),\n",
    "t2 as (\n",
    "  SELECT date(tpep_pickup_datetime) as date from taxi_trips where tpep_pickup_datetime >= '2014-01-01' and tpep_pickup_datetime <= '2014-12-31'\n",
    "  UNION ALL\n",
    "  SELECT date(pickup_datetime) as date from uber_trips where pickup_datetime >= '2014-01-01' and pickup_datetime <= '2014-12-31'\n",
    "), \n",
    "t3 as (\n",
    "  SELECT date, COUNT(*) as trip_count FROM t2 GROUP BY date\n",
    ")\n",
    "SELECT t1.date, t3.trip_count FROM t1 LEFT JOIN t3 on t1.date = t3.date\n",
    "\"\"\"\n",
    "query5_result = conn.execute(text(QUERY_5)).fetchall()\n",
    "query5_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a9fb81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf493d",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "During Hurricane Sandy in NYC (Oct 29-30, 2012), plus the week leading up and the week after,\n",
    "how many trips were taken each hour, and for each hour, how much precipitation did NYC\n",
    "receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bab24eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2012-10-22', 0, 5, 0.0, 7.0),\n",
       " ('2012-10-22', 1, None, 0.0, 5.0),\n",
       " ('2012-10-22', 2, 2, 0.0, 7.0),\n",
       " ('2012-10-22', 3, 2, 0.0, 0.0),\n",
       " ('2012-10-22', 4, 2, 0.0, 0.0),\n",
       " ('2012-10-22', 5, 1, 0.0, 0.0),\n",
       " ('2012-10-22', 6, 8, 0.0, 5.0),\n",
       " ('2012-10-22', 7, 7, 0.0, 3.0),\n",
       " ('2012-10-22', 8, 4, 0.0, 3.0),\n",
       " ('2012-10-22', 9, 8, 0.0, 5.0),\n",
       " ('2012-10-22', 12, 12, 0.0, 11.0),\n",
       " ('2012-10-22', 14, 3, 0.0, 7.0),\n",
       " ('2012-10-22', 15, 5, 0.0, 6.0),\n",
       " ('2012-10-22', 16, 10, 0.0, 3.0),\n",
       " ('2012-10-22', 17, 13, 0.0, 7.0),\n",
       " ('2012-10-22', 18, 11, 0.0, 5.0),\n",
       " ('2012-10-22', 19, 7, 0.0, 5.0),\n",
       " ('2012-10-22', 20, 11, 0.0, 3.0),\n",
       " ('2012-10-22', 21, 5, 0.0, 0.0),\n",
       " ('2012-10-22', 22, 12, 0.0, 3.0),\n",
       " ('2012-10-22', 23, 4, 0.0, 3.0),\n",
       " ('2012-10-23', 0, 3, 0, 3.0),\n",
       " ('2012-10-23', 1, 2, 0, 0.0),\n",
       " ('2012-10-23', 2, 2, 0, 3.0),\n",
       " ('2012-10-23', 3, None, 0, 0.0),\n",
       " ('2012-10-23', 4, None, 0, 3.0),\n",
       " ('2012-10-23', 5, 2, 0, 0.0),\n",
       " ('2012-10-23', 6, 5, 0, 0.0),\n",
       " ('2012-10-23', 7, 12, 0, 0.0),\n",
       " ('2012-10-23', 8, 15, 0, 0.0),\n",
       " ('2012-10-23', 9, 6, 0, 3.0),\n",
       " ('2012-10-23', 10, 9, 0, 0.0),\n",
       " ('2012-10-23', 11, 10, 0, 3.0),\n",
       " ('2012-10-23', 12, 8, 0.0, 0.0),\n",
       " ('2012-10-23', 16, 3, 0, 3.0),\n",
       " ('2012-10-23', 18, 14, 0, 5.0),\n",
       " ('2012-10-23', 19, 7, 0.0, 0.0),\n",
       " ('2012-10-23', 20, 18, 0.02, 0.0),\n",
       " ('2012-10-23', 21, 9, 0.0, 5.0),\n",
       " ('2012-10-23', 22, 13, 0.01, 0.0),\n",
       " ('2012-10-23', 23, 11, 0.0, 5.0),\n",
       " ('2012-10-24', 0, 1, 0.0, 3.0),\n",
       " ('2012-10-24', 1, 2, 0, 6.0),\n",
       " ('2012-10-24', 2, 4, 0, 5.0),\n",
       " ('2012-10-24', 3, None, 0.0, 7.0),\n",
       " ('2012-10-24', 4, 3, 0.0, 7.0),\n",
       " ('2012-10-24', 5, 5, 0.0, 6.0),\n",
       " ('2012-10-24', 6, 1, 0, 5.0),\n",
       " ('2012-10-24', 7, 8, 0, 5.0),\n",
       " ('2012-10-24', 8, 7, 0.0, 0.0),\n",
       " ('2012-10-24', 9, 15, 0, 0.0),\n",
       " ('2012-10-24', 10, 6, 0.0, 7.0),\n",
       " ('2012-10-24', 11, 5, 0.0, 7.0),\n",
       " ('2012-10-24', 12, 7, 0, 8.0),\n",
       " ('2012-10-24', 13, 7, 0.0, 8.0),\n",
       " ('2012-10-24', 14, 6, 0, 6.0),\n",
       " ('2012-10-24', 15, 7, 0.0, 7.0),\n",
       " ('2012-10-24', 16, 7, 0.0, 8.0),\n",
       " ('2012-10-24', 17, 12, 0.0, 5.0),\n",
       " ('2012-10-24', 18, 7, 0.0, 7.0),\n",
       " ('2012-10-24', 19, 6, 0.0, 8.0),\n",
       " ('2012-10-24', 20, 12, 0, 0.0),\n",
       " ('2012-10-24', 21, 17, 0, 3.0),\n",
       " ('2012-10-24', 22, 9, 0, 5.0),\n",
       " ('2012-10-24', 23, 7, 0.0, 0.0),\n",
       " ('2012-10-25', 0, 7, 0, 6.0),\n",
       " ('2012-10-25', 1, 2, 0.0, 3.0),\n",
       " ('2012-10-25', 2, 3, 0, 3.0),\n",
       " ('2012-10-25', 3, None, 0, 6.0),\n",
       " ('2012-10-25', 4, 1, 0.0, 6.0),\n",
       " ('2012-10-25', 5, 3, 0.0, 0.0),\n",
       " ('2012-10-25', 6, 6, 0.0, 5.0),\n",
       " ('2012-10-25', 7, 6, 0.0, 6.0),\n",
       " ('2012-10-25', 8, 13, 0.0, 5.0),\n",
       " ('2012-10-25', 9, 8, 0.0, 3.0),\n",
       " ('2012-10-25', 10, 7, 0.0, 6.0),\n",
       " ('2012-10-25', 11, 12, 0.0, 0.0),\n",
       " ('2012-10-25', 12, 9, 0.0, 6.0),\n",
       " ('2012-10-25', 13, 8, 0.0, 0.0),\n",
       " ('2012-10-25', 14, 10, 0.0, 5.0),\n",
       " ('2012-10-25', 15, 8, 0.0, 5.0),\n",
       " ('2012-10-25', 16, 3, 0.0, 0.0),\n",
       " ('2012-10-25', 17, 5, 0.0, 3.0),\n",
       " ('2012-10-25', 18, 9, 0.0, 0.0),\n",
       " ('2012-10-25', 19, 6, 0.0, 0.0),\n",
       " ('2012-10-25', 20, 18, 0.0, 3.0),\n",
       " ('2012-10-25', 21, 16, 0.0, 3.0),\n",
       " ('2012-10-25', 22, 12, 0.0, 3.0),\n",
       " ('2012-10-25', 23, 10, 0.0, 0.0),\n",
       " ('2012-10-26', 0, 6, 0, 0.0),\n",
       " ('2012-10-26', 1, 4, 0.0, 0.0),\n",
       " ('2012-10-26', 2, 3, 0.0, 0.0),\n",
       " ('2012-10-26', 3, 4, 0.0, 3.0),\n",
       " ('2012-10-26', 4, 2, 0.0, 0.0),\n",
       " ('2012-10-26', 5, 2, 0.0, 0.0),\n",
       " ('2012-10-26', 6, None, 0.0, 0.0),\n",
       " ('2012-10-26', 7, 6, 0.0, 3.0),\n",
       " ('2012-10-26', 8, 4, 0.0, 3.0),\n",
       " ('2012-10-26', 9, 9, 0.0, 3.0),\n",
       " ('2012-10-26', 10, 8, 0.0, 3.0),\n",
       " ('2012-10-26', 11, 9, 0.0, 3.0),\n",
       " ('2012-10-26', 12, 11, 0.0, 0.0),\n",
       " ('2012-10-26', 13, 10, 0.0, 3.0),\n",
       " ('2012-10-26', 14, 9, 0.0, 3.0),\n",
       " ('2012-10-26', 15, 8, 0.0, 0.0),\n",
       " ('2012-10-26', 16, 3, 0.0, 0.0),\n",
       " ('2012-10-26', 17, 8, 0.0, 0.0),\n",
       " ('2012-10-26', 18, 9, 0.0, 0.0),\n",
       " ('2012-10-26', 19, 11, 0.0, 0.0),\n",
       " ('2012-10-26', 20, 18, 0.0, 3.0),\n",
       " ('2012-10-26', 21, 8, 0.0, 3.0),\n",
       " ('2012-10-26', 22, 13, 0.0, 0.0),\n",
       " ('2012-10-26', 23, 11, 0.0, 0.0),\n",
       " ('2012-10-27', 0, 12, 0.0, 3.0),\n",
       " ('2012-10-27', 1, 9, 0.0, 0.0),\n",
       " ('2012-10-27', 2, 6, 0.0, 3.0),\n",
       " ('2012-10-27', 3, 4, 0.0, 0.0),\n",
       " ('2012-10-27', 4, 1, 0.0, 6.0),\n",
       " ('2012-10-27', 5, 2, 0.0, 6.0),\n",
       " ('2012-10-27', 6, 3, 0.0, 6.0),\n",
       " ('2012-10-27', 7, 3, 0, 5.0),\n",
       " ('2012-10-27', 8, 3, 0, 5.0),\n",
       " ('2012-10-27', 9, 11, 0.0, 6.0),\n",
       " ('2012-10-27', 10, 9, 0, 7.0),\n",
       " ('2012-10-27', 11, 6, 0, 5.0),\n",
       " ('2012-10-27', 12, 10, 0.0, 8.0),\n",
       " ('2012-10-27', 13, 6, 0.0, 8.0),\n",
       " ('2012-10-27', 14, 6, 0.0, 10.0),\n",
       " ('2012-10-27', 15, 8, 0.0, 10.0),\n",
       " ('2012-10-27', 16, 14, 0.0, 7.0),\n",
       " ('2012-10-27', 17, 12, 0.0, 7.0),\n",
       " ('2012-10-27', 18, 12, 0.0, 7.0),\n",
       " ('2012-10-27', 19, 12, 0, 8.0),\n",
       " ('2012-10-27', 20, 16, 0.0, 7.0),\n",
       " ('2012-10-27', 21, 9, 0.0, 9.0),\n",
       " ('2012-10-27', 22, 15, 0.0, 9.0),\n",
       " ('2012-10-27', 23, 15, 0.0, 8.0),\n",
       " ('2012-10-28', 0, 16, 0.0, 11.0),\n",
       " ('2012-10-28', 1, 13, 0.0, 8.0),\n",
       " ('2012-10-28', 2, 9, 0.0, 8.0),\n",
       " ('2012-10-28', 3, 5, 0.0, 9.0),\n",
       " ('2012-10-28', 4, 12, 0.0, 10.0),\n",
       " ('2012-10-28', 5, 3, 0.0, 11.0),\n",
       " ('2012-10-28', 6, 1, 0, 10.0),\n",
       " ('2012-10-28', 7, 6, 0.0, 11.0),\n",
       " ('2012-10-28', 8, 3, 0.0, 11.0),\n",
       " ('2012-10-28', 9, 8, 0.0, 11.0),\n",
       " ('2012-10-28', 10, 6, 0.0, 10.0),\n",
       " ('2012-10-28', 11, 9, 0.0, 8.0),\n",
       " ('2012-10-28', 12, 6, 0.0, 7.0),\n",
       " ('2012-10-28', 13, 6, 0.0, 13.0),\n",
       " ('2012-10-28', 14, 10, 0.0, 13.0),\n",
       " ('2012-10-28', 15, 13, 0.0, 13.0),\n",
       " ('2012-10-28', 16, 10, 0.0, 16.0),\n",
       " ('2012-10-28', 17, 6, 0.0, 11.0),\n",
       " ('2012-10-28', 18, 7, 0.0, 15.0),\n",
       " ('2012-10-28', 19, 5, 0.0, 14.0),\n",
       " ('2012-10-28', 20, 6, 0.0, 16.0),\n",
       " ('2012-10-28', 21, 5, 0.0, 14.0),\n",
       " ('2012-10-28', 22, 4, 0.0, 16.0),\n",
       " ('2012-10-28', 23, 2, 0.0, 14.0),\n",
       " ('2012-10-29', 0, 2, 0.0, 16.0),\n",
       " ('2012-10-29', 1, None, 0.0, 11.0),\n",
       " ('2012-10-29', 2, 1, 0.0, 13.0),\n",
       " ('2012-10-29', 3, None, 0.0, 17.0),\n",
       " ('2012-10-29', 4, 2, 0.0, 15.0),\n",
       " ('2012-10-29', 5, 1, 0.0, 15.0),\n",
       " ('2012-10-29', 6, None, 0.02, 16.0),\n",
       " ('2012-10-29', 7, 3, 0.02, 17.0),\n",
       " ('2012-10-29', 8, 3, 0.0, 21.0),\n",
       " ('2012-10-29', 9, 2, 0.0, 16.0),\n",
       " ('2012-10-29', 11, 7, 0, 21.0),\n",
       " ('2012-10-29', 12, 6, 0.02, 15.0),\n",
       " ('2012-10-29', 13, None, 0.02, 24.0),\n",
       " ('2012-10-29', 14, 3, 0.03, 23.0),\n",
       " ('2012-10-29', 15, 3, 0.07, 26.0),\n",
       " ('2012-10-29', 16, 1, 0.1, 23.0),\n",
       " ('2012-10-29', 17, 1, 0.04, 29.0),\n",
       " ('2012-10-29', 18, 4, 0.02, 21.0),\n",
       " ('2012-10-29', 19, None, 0.01, 25.0),\n",
       " ('2012-10-29', 20, None, 0.0, 17.0),\n",
       " ('2012-10-29', 21, 1, 0.0, 15.0),\n",
       " ('2012-10-29', 22, None, 0.02, 9.0),\n",
       " ('2012-10-29', 23, 1, 0.03, 7.0),\n",
       " ('2012-10-30', 0, 1, 0.03, 13.0),\n",
       " ('2012-10-30', 1, None, 0.0, 13.0),\n",
       " ('2012-10-30', 2, None, 0.03, 9.0),\n",
       " ('2012-10-30', 3, None, 0.04, 17.0),\n",
       " ('2012-10-30', 4, 1, 0.0, 9.0),\n",
       " ('2012-10-30', 5, None, 0.01, 7.0),\n",
       " ('2012-10-30', 6, None, 0.01, 7.0),\n",
       " ('2012-10-30', 7, None, 0.0, 10.0),\n",
       " ('2012-10-30', 8, None, 0.01, 11.0),\n",
       " ('2012-10-30', 9, 3, 0.01, 15.0),\n",
       " ('2012-10-30', 10, 3, 0.02, 8.0),\n",
       " ('2012-10-30', 11, 3, 0.0, 7.0),\n",
       " ('2012-10-30', 12, 4, 0, 9.0),\n",
       " ('2012-10-30', 13, 4, 0, 7.0),\n",
       " ('2012-10-30', 16, 6, 0.01, 3.0),\n",
       " ('2012-10-30', 17, 5, 0, 6.0),\n",
       " ('2012-10-30', 18, 6, 0.0, 5.0),\n",
       " ('2012-10-30', 19, 10, 0.0, 3.0),\n",
       " ('2012-10-30', 20, 9, 0, 0.0),\n",
       " ('2012-10-30', 21, 6, 0.0, 5.0),\n",
       " ('2012-10-30', 22, 9, 0.0, 7.0),\n",
       " ('2012-10-30', 23, 3, 0, 5.0),\n",
       " ('2012-10-31', 0, 2, 0.0, 3.0),\n",
       " ('2012-10-31', 1, 1, 0.01, 5.0),\n",
       " ('2012-10-31', 2, 4, 0.0, 0.0),\n",
       " ('2012-10-31', 3, None, 0.0, 8.0),\n",
       " ('2012-10-31', 5, None, 0.0, 0.0),\n",
       " ('2012-10-31', 6, 2, 0.0, 6.0),\n",
       " ('2012-10-31', 8, 2, 0, 6.0),\n",
       " ('2012-10-31', 9, 3, 0, 6.0),\n",
       " ('2012-10-31', 10, 3, 0, 5.0),\n",
       " ('2012-10-31', 11, 3, 0.0, 5.0),\n",
       " ('2012-10-31', 12, 1, 0, 9.0),\n",
       " ('2012-10-31', 13, 3, 0.0, 6.0),\n",
       " ('2012-10-31', 14, 4, 0.0, 5.0),\n",
       " ('2012-10-31', 15, 3, 0.0, 3.0),\n",
       " ('2012-10-31', 16, 2, 0.0, 5.0),\n",
       " ('2012-10-31', 17, 4, 0.0, 5.0),\n",
       " ('2012-10-31', 18, 9, 0.0, 3.0),\n",
       " ('2012-10-31', 19, 3, 0.0, 9.0),\n",
       " ('2012-10-31', 20, 7, 0.0, 7.0),\n",
       " ('2012-10-31', 21, 3, 0.0, 7.0),\n",
       " ('2012-10-31', 22, 10, 0.0, 6.0),\n",
       " ('2012-10-31', 23, 6, 0.0, 3.0),\n",
       " ('2012-11-01', 0, 4, 0.0, 3.0),\n",
       " ('2012-11-01', 1, 3, 0.0, 3.0),\n",
       " ('2012-11-01', 2, 2, 0.0, 3.0),\n",
       " ('2012-11-01', 4, 3, 0.0, 7.0),\n",
       " ('2012-11-01', 5, 1, 0.0, 6.0),\n",
       " ('2012-11-01', 6, 4, 0.0, 13.0),\n",
       " ('2012-11-01', 8, 4, 0.0, 7.0),\n",
       " ('2012-11-01', 9, 4, 0.0, 3.0),\n",
       " ('2012-11-01', 10, 10, 0.0, 6.0),\n",
       " ('2012-11-01', 11, 9, 0.0, 6.0),\n",
       " ('2012-11-01', 12, 8, 0.0, 11.0),\n",
       " ('2012-11-01', 13, 7, 0.0, 8.0),\n",
       " ('2012-11-01', 14, 4, 0.0, 8.0),\n",
       " ('2012-11-01', 16, 4, 0.0, 5.0),\n",
       " ('2012-11-01', 17, 4, 0.0, 5.0),\n",
       " ('2012-11-01', 18, 10, 0.0, 9.0),\n",
       " ('2012-11-01', 19, 8, 0.0, 3.0),\n",
       " ('2012-11-01', 20, 7, 0.0, 5.0),\n",
       " ('2012-11-01', 21, 3, 0.0, 8.0),\n",
       " ('2012-11-01', 22, 6, 0.0, 5.0),\n",
       " ('2012-11-01', 23, 3, 0.0, 0.0),\n",
       " ('2012-11-02', 0, 4, 0.0, 5.0),\n",
       " ('2012-11-02', 1, 1, 0.0, 7.0),\n",
       " ('2012-11-02', 2, 2, 0.0, 3.0),\n",
       " ('2012-11-02', 3, 1, 0.0, 3.0),\n",
       " ('2012-11-02', 4, None, 0.0, 5.0),\n",
       " ('2012-11-02', 5, 3, 0.0, 5.0),\n",
       " ('2012-11-02', 6, 2, 0.0, 6.0),\n",
       " ('2012-11-02', 8, 4, 0.0, 5.0),\n",
       " ('2012-11-02', 9, 3, 0.0, 7.0),\n",
       " ('2012-11-02', 10, 5, 0.0, 9.0),\n",
       " ('2012-11-02', 11, 3, 0.0, 7.0),\n",
       " ('2012-11-02', 12, 6, 0.0, 7.0),\n",
       " ('2012-11-02', 13, 3, 0.0, 6.0),\n",
       " ('2012-11-02', 14, 5, 0.0, 6.0),\n",
       " ('2012-11-02', 15, 6, 0.0, 5.0),\n",
       " ('2012-11-02', 16, 4, 0.0, 11.0),\n",
       " ('2012-11-02', 17, 2, 0.0, 8.0),\n",
       " ('2012-11-02', 18, 1, 0.0, 9.0),\n",
       " ('2012-11-02', 19, 8, 0.0, 7.0),\n",
       " ('2012-11-02', 20, 7, 0.0, 9.0),\n",
       " ('2012-11-02', 21, 15, 0.0, 7.0),\n",
       " ('2012-11-02', 22, 7, 0.0, 8.0),\n",
       " ('2012-11-02', 23, 12, 0.0, 8.0),\n",
       " ('2012-11-03', 0, 11, 0.0, 7.0),\n",
       " ('2012-11-03', 1, 5, 0.0, 7.0),\n",
       " ('2012-11-03', 2, 8, 0.0, 7.0),\n",
       " ('2012-11-03', 3, 2, 0.0, 7.0),\n",
       " ('2012-11-03', 4, 1, 0.0, 8.0),\n",
       " ('2012-11-03', 5, 1, 0.0, 8.0),\n",
       " ('2012-11-03', 6, 1, 0.0, 7.0),\n",
       " ('2012-11-03', 7, 4, 0.0, 6.0),\n",
       " ('2012-11-03', 8, 2, 0.0, 10.0),\n",
       " ('2012-11-03', 9, 4, 0.0, 13.0),\n",
       " ('2012-11-03', 10, 6, 0.0, 6.0),\n",
       " ('2012-11-03', 11, 12, 0.0, 13.0),\n",
       " ('2012-11-03', 12, 5, 0.0, 13.0),\n",
       " ('2012-11-03', 13, 7, 0.0, 8.0),\n",
       " ('2012-11-03', 14, 11, 0.0, 8.0),\n",
       " ('2012-11-03', 15, 7, 0.0, 7.0),\n",
       " ('2012-11-03', 16, 8, 0.0, 10.0),\n",
       " ('2012-11-03', 17, 11, 0.0, 9.0),\n",
       " ('2012-11-03', 18, 11, 0.0, 9.0),\n",
       " ('2012-11-03', 19, 10, 0.0, 13.0),\n",
       " ('2012-11-03', 20, 7, 0.0, 10.0),\n",
       " ('2012-11-03', 21, 13, 0.0, 9.0),\n",
       " ('2012-11-03', 22, 9, 0.0, 0.0),\n",
       " ('2012-11-03', 23, 14, 0.0, 7.0),\n",
       " ('2012-11-04', 0, 11, 0.0, 9.0),\n",
       " ('2012-11-04', 1, 20, 0.0, 7.0),\n",
       " ('2012-11-04', 2, 9, 0.0, 7.0),\n",
       " ('2012-11-04', 3, 2, 0.0, 7.0),\n",
       " ('2012-11-04', 4, 5, 0.0, 8.0),\n",
       " ('2012-11-04', 5, 1, 0.0, 6.0),\n",
       " ('2012-11-04', 6, 3, 0.0, 6.0),\n",
       " ('2012-11-04', 7, 1, 0.0, 3.0),\n",
       " ('2012-11-04', 8, 3, 0.0, 7.0),\n",
       " ('2012-11-04', 9, 2, 0.0, 9.0),\n",
       " ('2012-11-04', 11, 8, 0.0, 6.0),\n",
       " ('2012-11-04', 12, 10, 0.0, 8.0),\n",
       " ('2012-11-04', 13, 8, 0.0, 8.0),\n",
       " ('2012-11-04', 14, 12, 0.0, 7.0),\n",
       " ('2012-11-04', 15, 4, 0.0, 7.0),\n",
       " ('2012-11-04', 16, 7, 0.0, 5.0),\n",
       " ('2012-11-04', 17, 11, 0.0, 5.0),\n",
       " ('2012-11-04', 19, 10, 0.0, 7.0),\n",
       " ('2012-11-04', 21, 3, 0.0, 7.0),\n",
       " ('2012-11-04', 22, 6, 0.0, 6.0),\n",
       " ('2012-11-04', 23, 2, 0.0, 5.0),\n",
       " ('2012-11-05', 0, 3, 0.0, 0.0),\n",
       " ('2012-11-05', 1, 2, 0.0, 5.0),\n",
       " ('2012-11-05', 2, None, 0.0, 3.0),\n",
       " ('2012-11-05', 3, None, 0.0, 7.0),\n",
       " ('2012-11-05', 4, None, 0.0, 3.0),\n",
       " ('2012-11-05', 5, 2, 0.0, 6.0),\n",
       " ('2012-11-05', 6, 3, 0.0, 8.0),\n",
       " ('2012-11-05', 7, 8, 0.0, 6.0),\n",
       " ('2012-11-05', 8, 9, 0.0, 7.0),\n",
       " ('2012-11-05', 9, 8, 0.0, 3.0),\n",
       " ('2012-11-05', 10, 6, 0.0, 3.0),\n",
       " ('2012-11-05', 11, 7, 0.0, 3.0),\n",
       " ('2012-11-05', 12, 12, 0.0, 5.0),\n",
       " ('2012-11-05', 13, 5, 0.0, 3.0),\n",
       " ('2012-11-05', 15, 6, 0.0, 8.0),\n",
       " ('2012-11-05', 17, 21, 0.0, 5.0),\n",
       " ('2012-11-05', 18, 10, 0.0, 5.0),\n",
       " ('2012-11-05', 19, 9, 0.0, 0.0),\n",
       " ('2012-11-05', 20, 12, 0.0, 3.0),\n",
       " ('2012-11-05', 21, 8, 0.0, 7.0),\n",
       " ('2012-11-05', 22, 4, 0.0, 6.0),\n",
       " ('2012-11-05', 23, 6, 0.0, 9.0)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_6_FILENAME = \"query6.sql\"\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "WITH t1 as (\n",
    "  SELECT date(date) as day, hour as time, sum(hourlyPrecipitation) as precipitation, avg(hourlyWindSpeed) as windSpeed FROM hourly_weather \n",
    "  WHERE date >= '2012-10-22' and date <= '2012-11-06'\n",
    "  GROUP BY day, time  \n",
    "),\n",
    "t2 as (\n",
    "  SELECT date(tpep_pickup_datetime) as date, hour as time from taxi_trips\n",
    "  WHERE date >= '2012-10-22' and date <= '2012-11-06'\n",
    "  UNION ALL\n",
    "  SELECT date(pickup_datetime) as date, hour as time from uber_trips\n",
    "  WHERE date >= '2012-10-22' and date <= '2012-11-06'\n",
    "), \n",
    "t3 as (\n",
    "  SELECT date, time, count(*) as trip_count from t2 group by date, time\n",
    ")\n",
    "SELECT t1.day, t1.time, t3.trip_count, t1.Precipitation, t1.windSpeed \n",
    "FROM t1 left join t3 on t1.time = t3.time and t1.day = t3.date\n",
    "ORDER BY t1.day\n",
    "\"\"\"\n",
    "query6_result = conn.execute(text(QUERY_6)).fetchall()\n",
    "query6_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d6a32a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
